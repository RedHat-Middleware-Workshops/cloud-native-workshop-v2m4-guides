= Lab3 - Evolving services with Serverless
:experimental:
:imagesdir: images

In our cloud-native application architecture, We now have multiple microservices in a *reactive* system. However, it’s not necessary that applications and services remain up and running 24 hours day. They only need to be running *on-demand*, when something needs to use the service. This is one of the reasons why *serverless* architectures have gained popularity.

* *Serverless* is often used interchangeably with the term _FaaS_ (Functions-as-a-Service). But serverless doesn’t mean that there is no server. In fact, there _are_ servers - a public cloud provider provides the servers that deploy, run, and manage your application.
* *Serverless computing* is an emerging category that represents a shift in the way developers build and deliver software systems. Abstracting application infrastructure away from the code can greatly simplify the development process while introducing new cost and efficiency benefits. Serverless computing and FaaS will play an important role in helping to define the next era of enterprise
IT, along with cloud-native services and the https://enterprisersproject.com/hybrid-cloud[hybrid cloud^].
* *Serverless platforms* provide APIs that allow users to run code snippets (functions, also called _actions_) and return the results of each function. Serverless platforms also provide endpoints to allow the developer to retrieve function results. These endpoints can be used as inputs for other functions, thereby providing a sequence (or chain) of related functions.

The serverless application enables DevOps teams to enjoy benefits like:

* Optimizing computing resources(i.e CPU, Memory)
* Autoscaling
* Simplifying CI/CD pipeline

=== Goals of this lab

The goal is to develop serverless applications on *Red Hat Runtimes* and deploy them on *OpenShift 4* using https://www.openshift.com/learn/topics/serverless[OpenShift Serverless^] with a cloud-native, continuous integration and delivery (CI/CD) Pipelines. In this lab, we’ll deploy the Payment Service as a Quarkus-based serverless application using Knative Serving, Istio, and Tekton
Pipelines. After this lab, you should end up with something like:

image::lab3-goal.png[goal, 700]

The Knative Kafka Event _source_ enables _Knative Eventing_ integration with Apache Kafka. When a message is produced in Apache Kafka, the Apache Kafka Event Source will consume the produced message and post that message to the corresponding event _sink_.

==== What is Red Hat OpenShift Serverless?

OpenShift Serverless helps developers to deploy and run applications that will scale up or scale to zero on-demand. Applications are packaged as OCI compliant Linux containers that can be run anywhere.

image::knative-serving-diagram.png[knative, 800]

Applications can be triggered by a variety of event sources, such as events from your own applications, cloud services from multiple providers, Software as a Service (SaaS) systems and Red Hat Services (https://access.redhat.com/products/red-hat-amq[AMQ Streams^]).

image::knative-eventing-diagram.png[knative, 800]

OpenShift Serverless applications can be integrated with other OpenShift services, such as OpenShift https://www.openshift.com/learn/topics/pipelines[Pipelines^], https://www.openshift.com/learn/topics/service-mesh[Service Mesh^], Monitoring and https://github.com/operator-framework/operator-metering[Metering^], delivering a complete serverless application development and deployment experience.

=== 1. Building a Native Executable

Let’s now produce a native executable for an example Quarkus application. It improves the startup time of the application, and produces a minimal disk and memory footprint, which is important in serverless applications. The executable would have everything to run the application including the `JVM`(shrunk to be just enough to run the application), and the application. This is accomplished using https://graalvm.org/[GraalVM^].

`GraalVM` is a universal virtual machine for compiling and running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Groovy, Kotlin, Clojure, and LLVM-based languages such as C and C++. It includes ahead-of-time compilation, aggressive dead code elimination, and optimal packaging as native binaries that moves a lot of startup logic to build-time, thereby reducing startup time and memory resource requirements significantly.

image::native-image-process.png[serverless, 700]

`GraalVM` is already installed for you. Inspect the value of the `GRAALVM_HOME` variable in the CodeReady Workspaces Terminal
with:

[source,sh,role="copypaste"]
----
echo $GRAALVM_HOME
----

In this step, we will learn how to compile the application to a native executable and run the native image on local machine.

Compiling a native image takes longer than a regular JAR file (bytecode) compilation. However, this compilation time is only incurred once, as opposed to every time the application starts, which is the case with other approaches for building and executing JARs.

Let’s find out why Quarkus calls itself _SuperSonic Subatomic Subatomic Java_. Let’s build a sample app. In CodeReady Terminal, run this command:

[source,sh,role="copypaste"]
----
mkdir /tmp/hello && cd /tmp/hello && \
mvn com.redhat.quarkus.platform:quarkus-maven-plugin:2.2.5.Final-redhat-00007:create \
    -DprojectGroupId=com.redhat.quarkus.platform \
    -DprojectArtifactId=getting-started \
    -DplatformGroupId=com.redhat.quarkus.platform \
    -DplatformVersion=2.2.5.Final-redhat-00007 \
    -DclassName="org.acme.quickstart.GreetingResource" \
    -Dpath="/hello"
----

This will create a simple Quarkus app in the */tmp/hello* directory.

Next, create a `native executable` with this command:

[source,sh,role="copypaste"]
----
mvn -f /tmp/hello/getting-started/pom.xml clean package -Pnative -DskipTests
----

[NOTE]
====
You might have _OutofMemory_ error during the native compilation. For that case, you need to append *-Dquarkus.native.native-image-xmx=2g* option to the maven command.
====

This may take a minute or two to run. One of the benefits of Quarkus is amazingly fast startup time, at the expense of a longer build time to optimize and remove dead code, process annotations, etc. This is only incurred once, at build time rather than _every_ startup!

[NOTE]
====
Since we are on Linux in this environment, and the OS that will eventually run our application is also Linux, we can use our local OS to build the native Quarkus app. If you need to build native Linux binaries when on other OS’s like Windows or Mac OS X, you’ll need to have Docker installed and then use `mvn clean package -Pnative -Dnative-image.docker-build=true -DskipTests=true`.
====

[source,none]
----
[getting-started-1.0.0-SNAPSHOT-runner:2235]     (inline):  49,486.05 ms,  2.28 GB
[getting-started-1.0.0-SNAPSHOT-runner:2235]    (compile): 364,993.56 ms,  2.40 GB
[getting-started-1.0.0-SNAPSHOT-runner:2235]      compile: 522,794.93 ms,  2.40 GB
[getting-started-1.0.0-SNAPSHOT-runner:2235]        image:  28,483.78 ms,  2.43 GB
[getting-started-1.0.0-SNAPSHOT-runner:2235]        write:   2,939.11 ms,  2.43 GB
[getting-started-1.0.0-SNAPSHOT-runner:2235]      [total]: 1,034,480.54 ms,  2.43 GB
# Printing build artifacts to: /tmp/hello/getting-started/target/getting-started-1.0.0-SNAPSHOT-native-image-source-jar/getting-started-1.0.0-SNAPSHOT-runner.build_artifacts.txt
[INFO] [io.quarkus.deployment.pkg.steps.NativeImageBuildRunner] objcopy --strip-debug getting-started-1.0.0-SNAPSHOT-runner
[INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 1056916ms
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  3:26 min
[INFO] Finished at: 2021-12-14T03:46:57Z
[INFO] ------------------------------------------------------------------------
[jboss@workspaceqs6f4a11ah12c3wa hello]$ 
----

The output of the native build is a native Linux binary, which you can see using the `readelf` command. Run this in a Terminal:

[source,sh,role="copypaste"]
----
readelf -h /tmp/hello/getting-started/target/*-runner
----

you'll see

[source,console]
----
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF64
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           Advanced Micro Devices X86-64
  Version:                           0x1
  ....
----

It's a binary that can only run on Linux, but as you'll see in a moment, this native executable starts up _very_ fast and takes up little memory.

Since our environment here is Linux, you can just run it. In the CodeReady Workspaces Terminal, run:

[source,sh,role="copypaste"]
----
/tmp/hello/getting-started/target/*-runner
----

[WARNING]
====
If you still have a previous application running in a separate CodeReady Terminal, you may get an error like `java.net.BindException: Address already in use`. Go to that other tab and press kbd:[CTRL+C] to stop the previous application, then try to run the native application again!
====

Notice the amazingly fast startup time:

[source,shell]
----
__  ____  __  _____   ___  __ ____  ______ 
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
INFO  [io.quarkus] (main) getting-started 1.0.0-SNAPSHOT native (powered by Quarkus xx.xx.xx) started in 0.023s. Listening on: http://0.0.0.0:8080
INFO  [io.quarkus] (main) Profile prod activated. 
INFO  [io.quarkus] (main) Installed features: [cdi, resteasy, smallrye-context-propagation, vertx]
----

That’s *23 milliseconds* to start up. The start-up time might be different in your environment.

And extremely low memory usage as reported by the Linux `ps` utility. While the app is running, run the following command in another Terminal:

[source,sh,role="copypaste"]
----
ps -o pid,rss,command -p $(pgrep -f runner)
----

You should see something like:

[source,shell]
----
    PID   RSS COMMAND
   2320 41396 /tmp/hello/getting-started/target/getting-started-1.0.0-SNAPSHOT-runner
----

This shows that our process is taking around 41MB of memory (https://en.wikipedia.org/wiki/Resident_set_size[Resident Set
Size^], or RSS). Pretty compact!

[NOTE]
====
The RSS and memory usage of any app, including Quarkus, will vary depending your specific environment, and will rise as the application experiences load.
====

Make sure the app works. In a new CodeReady Workspaces Terminal run:

[source,sh,role="copypaste"]
----
curl -i http://localhost:8080/hello
----

You should see the return:

[source,console]
----
HTTP/1.1 200 OK
Content-Type: text/plain;charset=UTF-8
content-length: 14

Hello RESTEasy
----

*Congratulations!* You’ve now built a Java application as a native executable JAR and a Linux native binary. We’ll explore the benefits of native binaries later in when we start deploying to Kubernetes.

Be sure to terminate the running Quarkus development via kbd:[CTRL+C]).

=== 2. Delete old payment service

_OpenShift Serverless_ builds on Knative Serving to support deploying and serving of serverless applications and functions. _Serverless_ is easy to get started with and scales to support advanced scenarios.

The OpenShift Serverless provides middleware primitives that enable:

* Rapid deployment of serverless containers
* Automatic scaling up and down to zero
* Routing and network programming for Istio components
* Point-in-time snapshots of deployed code and configurations

In the lab, _OpenShift Serverless Operator_ is already installed on your OpenShift 4 cluster but if you want to install it on your
own OpenShift cluster, follow https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html/serverless/install#install-serverless-operator[Installing OpenShift Serverless^].

First, we need to delete existing `BuildConfig` as it is based an excutable Jar that we deployed it in the previous lab.

[source,sh,role="copypaste"]
----
oc delete bc/payment imagestream.image.openshift.io/payment
----

We also will delete our existing payment _deployment_ and _route_ since Knative will handle deploying the payment service and routing traffic to its managed pod when needed. Delete the existing payment deployment and its associated route and service with:

[source,sh,role="copypaste"]
----
oc delete dc/payment route/payment svc/payment
----

=== 3. Enable Knative Eventing integration with Apache Kafka Event

_Knative Eventing_ is a system that is designed to address a common need for cloud native development and provides composable primitives to enable `late-binding` event sources and event consumers with below goals:

* Services are loosely coupled during development and deployed independently.
* Producer can generate events before a consumer is listening, and a consumer can express an interest in an event or class of events that is not yet being produced.
* Services can be connected to create new applications without modifying producer or consumer, and with the ability to select a specific subset of events from a particular producer.

The _Apache Kafka Event source_ enables Knative Eventing integration with Apache Kafka. When a message is produced to Apache Kafka, the Event Source will consume the produced message and post that message to the corresponding event sink.

Remove direct Knative integration code. Currently our Payment service directly binds to Kafka to listen for events. Now that we have Knative eventing integration, we no longer need this code. Open the `PaymentResource.java` file (in `payment-service/src/main/java/com/redhat/cloudnative` directory).

Comment out the `onMessage()` method via kbd:[CTRL+/] (or kbd:[Command+/] on Mac OS):

[source,java]
----
//    @Incoming("orders")
//    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
//            throws IOException {
//
//        log.info("Kafka message with value = {} arrived", message.getPayload());
//        handleCloudEvent(message.getPayload());
//        return message.ack();
//    }
----

And delete the configuration for the incoming stream. In `application.properties`, comment out the following lines for the _Incoming_ stream and _OpenShift extension_ via kbd:[CTRL+/] (or kbd:[Command+/] on Mac OS):

[source,none]
----
# OpenShift extension
# quarkus.kubernetes-client.trust-certs=true
# quarkus.kubernetes.deploy=true
# quarkus.kubernetes.deployment-target=openshift
# quarkus.openshift.build-strategy=docker
# quarkus.openshift.expose=true

...

# Incoming stream (unneeded when using Knative events)
# mp.messaging.incoming.orders.connector=smallrye-kafka
# mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
# mp.messaging.incoming.orders.group.id=payment-order-service
# mp.messaging.incoming.orders.auto.offset.reset=earliest
# mp.messaging.incoming.orders.enable.auto.commit=true
# mp.messaging.incoming.orders.request.timeout.ms=30000
----

[WARNING]
====
Make sure you only comment out or delete the lines starting with `mp.messaging.incoming`, `OpenShift extension` and leave the rest!
====

Then append the following configuration for native compilation using `Mandrel` builder image:

[source,properties,role="copypaste"]
----
quarkus.container-image.group={{ USER_ID }}-cloudnativeapps// <1>
quarkus.container-image.registry=image-registry.openshift-image-registry.svc:5000
quarkus.kubernetes-client.trust-certs=true
quarkus.kubernetes.deployment-target=knative// <2>
quarkus.kubernetes.deploy=true
quarkus.openshift.build-strategy=docker
quarkus.openshift.expose=true
----

<1> Define a project name where you deploy a serverless application
<2> Enable the generation of Knative resources

[NOTE]
====
https://github.com/graalvm/mandrel[Mandrel^] is a downstream distribution of the GraalVM community edition. Mandrel's main goal is to provide a native-image release specifically to support https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus[Quarkus^]. The aim is to align the native-image capabilities from GraalVM with OpenJDK and Red Hat Enterprise Linux libraries to improve maintainability for native Quarkus applications.

You might have _OutofMemory_ error during the native compilation. For that case, you need to append *quarkus.native.native-image-xmx=2g* for setting the maximum Java heap in application.properties.
====

Rebuild and re-deploy new payment service via running the following maven plugin in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -Pnative -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

The _-Pnative_ argument selects the native maven profile which invokes the _Graal compiler_. It will take a few minutes(up to 7 mins) to complete a native binary build as well as deploying a new knative service. After successful creation of the service we should see a *Knative Service*(_KSVC_) and *Revision*(_REV_) in the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View]:

[NOTE]
====
It will take a few moments (up to 30 seconds) to fully render while the networking is setup properly. Try reloading the browser page if all you see is an empty box!
====

Let's edit labels to add _Quarkus_ icon. Previously you did this with a series of `oc label` commands, but let's do it once manually just for fun. Click on payment *REV* then select *Edit Labels* in _Actions_ drop box:

image::kservice-up.png[serverless, 700]

Add this label and click on *Save*:

[source,sh,role="copypaste"]
----
app.openshift.io/runtime=quarkus
----

image::quarkus-label.png[serverless, 500]

Now you see Quarkus icon in _Payment Service_ on topology:

image::kservice-up-quarkus.png[serverless, 700]

In the lab environment, _OpenShift Serverless_ will automatically scale services down to zero instances when the service(i.e. payment) has no request after *30 seconds* which means the payment service pod will unavailable in 30 seconds. Visit again the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^]. Ensure there's no *blue circle* in the payment service!

[NOTE]
====
You need to wait ~30 seconds before it scales to 0!
====

image::kservice-down.png[serverless, 700]

If you send traffic to this endpoint it will trigger the autoscaler to scale the app up. Click on http://payment-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN }}[Open URL^] to _trigger_ the payment service. This will send some dummy data to the `payment` service, but more importantly it triggered knative to spin up the pod again automatically, and will shut it down 30 seconds later.

image::payment-serving-magic.png[serverless, 700]

*Congratulations!* You’ve now deployed the payment service as a Quarkus native image, served with _OpenShift Serverless_, quicker than traditional Java applications. This is not the end of Serverless capabilites so we will now see how the payment service will scale up _magically_ in the following exercises.

Let's move on to create *KafkaSource* to enable *Knative Eventing*. In this lab, _Knative Eventing_ is already installed via the _Knative Eventing Operator_ in OpenShift 4 cluster.

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^], click on `+` icon on the right top corner.

image::plus-icon.png[serverless, 500]

Copy the following `KafkaSource` in `YAML` editor then click on *Create*:

[source,yaml,role="copypaste"]
----
apiVersion: sources.knative.dev/v1beta1
kind: KafkaSource
metadata:
  name: kafka-source
spec:
  consumerGroup: knative-group
  bootstrapServers:
  - my-cluster-kafka-bootstrap.{{ USER_ID }}-cloudnativeapps:9092
  topics:
  - orders
  sink:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: payment
----

You can see a new connection between Kafka and our *payments* service:

[NOTE]
====
Serverless is an advanced and still evolving feature, and in this case you may not see the `KafkaSource` represented on the Topology view depending on the version of OpenShift and OpenShift serverless in use. If it does not appear, do not worry! The underlying technology will work as expected, so simply continue on.
====

image::kafka-event-source-link.png[serverless, 700]

*Great job!* Let’s make sure if the payment service works properly with _Serverless_ features via Coolstore Web UI.

=== 4. End to End Functional Testing

Before getting started, we need to make sure if _payment service_ is scaled down to _zero_ again in {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^]:

image::payment-down-again.png[serverless, 700]

Let’s go shopping! Access the http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^]!

Add some cool items to your shopping cart in the following shopping scenarios:

[arabic]
. Add a _Pronounced Kubernetes_ to your cart by click on *Add to Cart*. You will see the `Success! Added!` message under the top menu.

image::add-to-cart-serverless.png[serverless, 1000]

[arabic, start=2]
. Go to the *Cart* tab and click on the *Checkout* button . Input the credit card information. The Card Info should be 16 digits and begin with the digit `4`. For example `4123987754646678`.

image::checkout-serverless.png[serverless, 1000]

[arabic, start=3]
. Input your Credit Card information to pay for the items:

image::input-cc-info-serverless.png[serverless, 1000]

[arabic, start=4]
. Let’s find out how _Kafka Event_ enables _Knative Eventing_. Go back to {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] then confirm if *payment service* is up automatically. It’s `MAGIC!!`

image::payment-serving-magic.png[serverless, 500]

[arabic, start=5]
. Confirm the _Payment Status_ of the your shopping items in the *All Orders* tab. It should be `Processing`.

image::payment-processing-serverless.png[serverless, 1000]


[arabic, start=5]
. After a few moments, reload the *Orders* page to confirm that the Payment Status changed to `COMPLETED` or `FAILED`.

[NOTE]
====
If the status is still *Processing*, the order service is processing incoming Kafka messages and store them in MongoDB. Please reload the page a few times more.
====

image::payment-completedorfailed-serverless.png[serverless, 1000]

This is the same result as before, but using Knative eventing to make a more powerful event-driven system that can scale with demand.

=== 5. Creating Cloud-Native CI/CD Pipelines using Tekton

There’re lots of open source CI/CD tools to build, test, deploy, and manage cloud-native applications/microservices: from on-premise to private, public, and hybrid cloud. Each tool provides different features to integrate with existing platforms/systems. This sometimes makes it more complex for DevOps teams to be able to create the CI/CD pipelines and maintain them on Kubernetes clusters. The *Cloud-Native CI/CD Pipeline* should be defined and executed in the Kubernetes native way. For example, the pipeline can be specified as Kubernetes resources using YAML format.

*OpenShift Pipelines* is a cloud-native, continuous integration and delivery (CI/CD) solution for building pipelines using Tekton. Tekton is a flexible, Kubernetes-native, open-source CI/CD framework that enables automating deployments across multiple platforms (Kubernetes, serverless, VMs, etc) by abstracting away the underlying details with the below features:

* Standard CI/CD pipeline definition based on Tekton
* Build images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc
* Deploy applications to multiple platforms such as Kubernetes, serverless and VMs
* Easy to extend and integrate with existing tools
* Scale pipelines on-demand
* Portable across any Kubernetes platform
* Designed for microservices and decentralized teams
* Integrated with the OpenShift Developer Console

[NOTE]
====
OpenShift Pipelines project is as a Developer Preview release. Developer Preview releases have features and functionality that might not be fully tested. Customers are encouraged to use and provide feedback on Technical Preview releases. Red Hat does not commit to fixing any reported issues and the provided features may not be available in future releases.
====

In the lab, OpenShift Pipelines is already installed on OpenShift 4 cluster but if you want to install OpenShift Pipelines on your own OpenShift cluster, OpenShift Pipelines is provided as an add-on on top of OpenShift that can be installed via an operator available in the OpenShift OperatorHub.

In order to define a pipeline, you need to create _custom resources_ as listed below:

* *Task*: a reusable, loosely coupled number of steps that perform a specific task (e.g. building a container image)
* *Pipeline*: the definition of the pipeline and the tasks that it should perform
* *PipelineResource*: inputs (e.g. git repository) and outputs (e.g. image registry) to and out of a pipeline or task
* *TaskRun*: the execution and result (i.e. success or failure) of running an instance of task
* *PipelineRun*: the execution and result (i.e. success or failure) of running a pipeline

image::tekton-arch.png[serverless, 800]

For further details on pipeline concepts, refer to the https://github.com/tektoncd/pipeline/tree/master/docs#learn-more[Tekton documentation^] that provides an excellent guide for understanding various parameters and attributes available for defining pipelines.

The Tekton API enables functionality to be separated from configuration (e.g. Pipelines vs PipelineRuns) such that steps can be reusable, but it does not provide a mechanism to generate the resources (notably, PipelineRuns and PipelineResources) that encapsulate these configurations dynamically. Triggers extends the Tekton architecture with the following CRDs:

* *TriggerTemplate* - Templates resources to be created (e.g. Create PipelineResources and PipelineRun that uses them)
* *TriggerBinding* - Validates events and extracts payload fields
* *EventListener* - Connects TriggerBindings and TriggerTemplates into an addressable endpoint (the event sink). It uses the extracted event parameters from each TriggerBinding (and any supplied static parameters) to create the resources specified in the corresponding TriggerTemplate. It also optionally allows an external service to pre-process the event payload via the interceptor field.
* *ClusterTriggerBinding* - A cluster-scoped TriggerBinding

Using _tektoncd/triggers_ in conjunction with tektoncd/pipeline enables you to easily create full-fledged CI/CD systems where the execution is defined entirely through Kubernetes resources.

In this lab, we will walk you through pipeline concepts and how to create and run a CI/CD pipeline for building and deploying microservices on OpenShift Serverless platform.

Let's deploy a https://github.com/openshift-pipelines/vote-ui[frontend^] and https://github.com/openshift-pipelines/vote-api[backend^] to `{{ USER_ID }}-cloudnative-pipeline` project.

*Tasks* consist of a number of steps that are executed sequentially. _Tasks_ are executed/run by creating TaskRuns. A TaskRun will schedule a Pod. Each step is executed in a separate container within the same pod. They can also have inputs and outputs in order to interact with other tasks in the pipeline.

When a _task_ starts running, it starts a pod and runs each step sequentially in a separate container on the same pod. This task happens to have a single step, but tasks can have multiple steps, and, since they run within the same pod, they have access to the same volumes in order to cache files, access configmaps, secrets, etc. You can specify volume using workspace. It is recommended that Tasks uses at most one writeable Workspace. Workspace can be secret, pvc, config or emptyDir.

[NOTE]
====
Only the requirement for a git repository is declared on the task and not a specific git repository to be used. That allows _tasks_ to be reusable for multiple pipelines and purposes. You can find more examples of reusable _tasks_ in the https://github.com/tektoncd/catalog[Tekton Catalog^] and
https://github.com/openshift/pipelines-catalog[OpenShift Catalog^] repositories.
====

We will need to create two pre-defined Tekton Tasks that we'll use in our pipeline that know how to compile Java apps and deploy them. Install the `apply-manifests` and `update-deployment` Tekton tasks using the following command:

[source,sh,role="copypaste"]
----
oc project {{ USER_ID }}-cloudnative-pipeline &&
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/apply_manifests_task.yaml && oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/update_deployment_task.yaml
----

Let’s confirm if the *tasks* are installed properly using https://github.com/tektoncd/cli/releases[Tekton CLI^] that already installed in CodeReady Workspaces:

[source,sh,role="copypaste"]
----
tkn task list
----

You will see the following 2 tasks:

[source,sh]
----
NAME               AGE
apply-manifests   10 seconds ago
update-deployment 10 seconds ago
----

We will be using https://buildah.io/[buildah^] clusterTasks, which gets installed along with Operator. Operator installs few ClusterTask which you can see:

[source,sh,role="copypaste"]
----
tkn clustertasks ls | grep buildah
----

You will see the following clustertasks:

[source,sh]
----
buildah                    Buildah task builds...   14 minutes ago
buildah-1-6-0              Buildah task builds...   14 minutes ago
----

A pipeline defines a number of tasks that should be executed and how they interact with each other via their inputs and outputs.

In this lab, we will create a pipeline that takes the source code of the application from GitHub and then builds and deploys it on OpenShift.

image::pipeline-diagram.png[serverless, 800]

This pipeline helps you to build and deploy backend/frontend, using the configured resources. Here is a high level description of the pipeline:

<1> Clones the source code of the application from a git repository by referring (_git-url_ and _git-revision_ param)
<2> Builds the container image of application using the _buildah_ clustertask that uses Buildah to build the image
<3> The application image is pushed to an image registry by refering (_image_ param)
<4> The new application image is deployed on OpenShift using the _apply-manifests_ and _update-deployment_ tasks

[NOTE]
====
You might have noticed that there are no references to the git repository or the image registry it will be pushed to in pipeline. That's because pipeline in Tekton are designed to be generic and re-usable across environments and stages through the application's lifecycle.
====

Pipelines abstract away the specifics of the git source repository and image to be produced as _PipelineResources_ or _Params_. When triggering a pipeline, you can provide different git repositories and image registries to be used during pipeline execution. Be patient! You will do that in a little bit in the next section.

The execution order of _tasks_ is determined by dependencies that are defined between the tasks via *inputs* and *outputs* as well as explicit orders that are defined via *runAfter*.

_workspaces_ field allow you to specify one or more volumes that each Task in the Pipeline requires during execution. You specify one or more Workspaces in the _workspaces_ field.

In the OpenShift console, first be sure you're on the _Topology_ view, then use the drop down to select the `{{USER_ID}}-cloudnative-pipeline` project.

image::pipeline-project-select.png[serverless, 800]

Next, click the `+` button at the upper right, and paste in the following YAML and click *Create*:

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: build-and-deploy
spec:
  workspaces:
  - name: shared-workspace
  params:
  - name: deployment-name
    type: string
    description: name of the deployment to be patched
  - name: git-url
    type: string
    description: url of the git repo for the code of deployment
  - name: git-revision
    type: string
    description: revision to be used from repo of the code for deployment
    default: "pipelines-1.5"
  - name: IMAGE
    type: string
    description: image to be build from the code
  tasks:
  - name: fetch-repository
    taskRef:
      name: git-clone
      kind: ClusterTask
    workspaces:
    - name: output
      workspace: shared-workspace
    params:
    - name: url
      value: $(params.git-url)
    - name: subdirectory
      value: ""
    - name: deleteExisting
      value: "true"
    - name: revision
      value: $(params.git-revision)
  - name: build-image
    taskRef:
      name: buildah
      kind: ClusterTask
    params:
    - name: TLSVERIFY
      value: "false"
    - name: IMAGE
      value: $(params.IMAGE)
    workspaces:
    - name: source
      workspace: shared-workspace
    runAfter:
    - fetch-repository
  - name: apply-manifests
    taskRef:
      name: apply-manifests
    workspaces:
    - name: source
      workspace: shared-workspace
    runAfter:
    - build-image
  - name: update-deployment
    taskRef:
      name: update-deployment
    params:
    - name: deployment
      value: $(params.deployment-name)
    - name: IMAGE
      value: $(params.IMAGE)
    runAfter:
    - apply-manifests
----

You will see the pipeline you have created:

image::console-import-yaml-2.png[serverless, 800]

Now that the pipeline is created, check the list of pipelines you have created using the _Tekton CLI_ in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
tkn pipeline ls
----

You can see the list of resources created:

[source,shell]
----
NAME               AGE              LAST RUN   STARTED   DURATION   STATUS
build-and-deploy   32 seconds ago   ---        ---       ---        ---
----

You need to create the *PersistentVolumeClaim* which can be used for Pipeline execution:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/persistent_volume_claim.yaml
----

A *PipelineRun* is how you can start a pipeline and tie it to the Git and image resources that should be used for this specific invocation. Let's go to {{ CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] and click on *Start*:

image::pipeline-start.png[serverless, 800]

This dialog box is where you bind the final target values for the source repo of the _build_ step, and the name of the image to deploy in the _deploy_ step. Input the parameter and select resources as below then Click on *Start*:

* deployment-name: `pipelines-vote-api`
 * git-url: `https://github.com/openshift/pipelines-vote-api.git`
 * git-revision: `pipelines-1.5`
 * IMAGE: `image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/pipelines-vote-api`
 * shared-workspace: `source-pvc` in *PVC*

image::pipeline-start-popup.png[serverless, 700]

As soon as you started the *build-and-deploy* pipeline, a pipelinerun is instantiated and pods are created to execute the tasks that are defined in the pipeline. After a few minutes, the pipeline should finish successfully. You can hover over the steps to get a quick snapshot of the step's progress, or click on the steps to see detailed logs of the steps.

image::pipeline-complete.png[serverless, 800]

Let's run the pipeline to deploy `vote-ui` application. Go back to {{ CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] and click on *Start*. Then input the parameter and select resources as below then Click on *Start*:

 * deployment-name: `pipelines-vote-ui`
 * git-url: `https://github.com/openshift/pipelines-vote-ui.git`
 * git-revision: `pipelines-1.5`
 * IMAGE: `image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/pipelines-vote-ui`
 * shared-workspace: `source-pvc` in *PVC*

image::pipeline-start-popup2.png[serverless, 700]

Once the build completes, on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnative-pipeline[Topology View^], and click on *Open URL*. You should see that the Vote UI is successfully built and deployed.

image::voteui-openurl.png[serverless, 800]

=== Summary

In this module, we learned how to develop cloud-native applications using multiple Java runtimes (Quarkus and Spring Boot), Javascript (Node.js) and different datasources (i.e. PostgreSQL, MongoDB) to handle a variety of business use cases which implement real-time _request/response_ communication using REST APIs, high performing cacheable services using _Red Hat Data Grid_, event-driven/reactive shopping cart service using Apache Kafka in _Red Hat AMQ Streams_.

Next, we converted the payment service to be a *Serverless* application using _OpenShift Serverless_ with Knative.

Finally, we created a reusable CI/CD pipeline using OpenShift Pipelines.

*Red Hat Runtimes* enables enterprise developers to design the advanced cloud-native architecture and develop, build, deploy the cloud-native application on hybrid cloud on the *Red Hat OpenShift Container Platform*. *Congratulations!*

==== Additional Resources:

* https://learn.openshift.com/developing-with-quarkus/[Quarkus Tutorials Right in Your Browser^]
* https://developers.redhat.com/articles/quarkus-quick-start-guide-kubernetes-native-java-stack/[Quarkus Quickstart Guide]
* https://docs.openshift.com/container-platform/latest/serverless/serverless-getting-started.html[Getting started with OpenShift Serverless^]
* https://www.openshift.com/learn/topics/pipelines[Cloud-native CI/CD on OpenShift^]
* https://developers.redhat.com/topics/serverless-architecture/[Serverless Architecture Articles^]
