== Lab3 - Evolving services with Serverless
:experimental:

In our cloud-native application architecture, We now have multiple microservices in a *reactive* system. However, it’s not necessary our applications and services be up and running 24 hours day. They only need to be running *on-demand*, when something needs to use the service. This is one of the reasons why *serverless* architectures have gained popularity.

* *Serverless* is often used interchangeably with the term _FaaS_ (Functions-as-a-Service). But serverless doesn’t mean that there is no server. In fact, there _are_ servers - a public cloud provider provides the servers that deploy, run, and manage your application.
* *Serverless computing* is an emerging category that represents a shift in the way developers build and deliver software systems. Abstracting application infrastructure away from the code can greatly simplify the development process while introducing new cost and efficiency benefits. Serverless computing and FaaS will play an important role in helping to define the next era of enterprise
IT, along with cloud-native services and the https://enterprisersproject.com/hybrid-cloud[hybrid cloud,window=_blank].
* *Serverless platforms* provide APIs that allow users to run code snippets (functions, also called _actions_) and return the results of each function. Serverless platforms also provide endpoints to allow the developer to retrieve function results. These endpoints can be used as inputs for other functions, thereby providing a sequence (or chain) of related functions.

The severless application enables DevOps teams to enjoy benefits like:

* Optimizing computing resources(i.e CPU, Memory)
* Autoscaling
* Simplifying CI/CD pipeline

=== Goals of this lab

The goal is to develop serverless applications on *Red Hat Runtimes* and deploy them on *OpenShift 4* using https://www.openshift.com/learn/topics/serverless[OpenShift Serverless,window=_blank] with a cloud-native, continuous integration and delivery (CI/CD) Pipelines. In this lab, we’ll deploy the Payment Service as a Quarkus-based serverless application using Knative Serving, Istio, and Tekton
Pipelines. After this lab, you should end up with something like:

image::lab3-goal.png[goal, 700]

The Knative Kafka Event _source_ enables _Knative Eventing_ integration with Apache Kafka. When a message is produced in Apache Kafka, the Apache Kafka Event Source will consume the produced message and post that message to the corresponding event _sink_.

==== What is Red Hat OpenShift Serverless?

OpenShift Serverless helps developers to deploy and run applications that will scale up or scale to zero on-demand. Applications are packaged as OCI compliant Linux containers that can be run anywhere.

image::knative-serving-diagram.png[knative, 800]

Applications can be triggered by a variety of event sources, such as events from your own applications, cloud services from multiple providers, Software as a Service (SaaS) systems and Red Hat Services (https://access.redhat.com/products/red-hat-amq[AMQ Streams,window=_blank]).

image::knative-eventing-diagram.png[knative, 800]

OpenShift Serverless applications can be integrated with other OpenShift services, such as OpenShift https://www.openshift.com/learn/topics/pipelines[Pipelines,window=_blank], https://www.openshift.com/learn/topics/service-mesh[Service Mesh,window=_blank], Monitoring and https://github.com/operator-framework/operator-metering[Metering,window=_blank], delivering a complete serverless application development and deployment experience.

==== What is Red Hat OpenShift Service Mesh?

https://www.openshift.com/learn/topics/service-mesh[OpenShift Service Mesh] provides traffic monitoring, access control, discovery, security, resiliency, and other useful things to a group of services. https://www.redhat.com/en/topics/microservices/what-is-a-service-mesh[Istio,window=_blank] does all that, but it doesn’t require any changes to the code of any of those services. To make the magic happen, Istio deploys a proxy (called a _sidecar_) next to each service. All of the traffic meant for a service goes to the proxy, which uses policies to decide how, when, or if that traffic should go on to the service. _Istio_ also enables sophisticated DevOps techniques such as canary deployments, circuit breakers, fault injection, and more.

image::openshift_service_mesh.png[ossm, 500]

_Istio_ also moves operational aspects away from code development and into the domain of operations. Why should a developer be burdened with circuit breakers and fault injections and should they respond to them? Yes, but for handling and/or creating them? Take that out of your code and let your code focus on the underlying business domain.

=== 1. Building a Native Executable

Let’s now produce a native executable for an example Quarkus application. It improves the startup time of the application, and produces a minimal disk and memory footprint. The executable would have everything to run the application including the `JVM`(shrunk to be just enough to run the application), and the application. This is accomplished using https://graalvm.org/[GraalVM,window=_blank].

`GraalVM` is a universal virtual machine for compiling and running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Groovy, Kotlin, Clojure, and LLVM-based languages such as C and C++. It includes ahead-of-time compilation, aggressive dead code elimination, and optimal packaging as native binaries that moves a lot of startup logic to build-time, thereby reducing startup time and memory resource requirements significantly.

image::native-image-process.png[serverless, 700]

`GraalVM` is already installed for you. Inspect the value of the `GRAALVM_HOME` variable in the CodeReady Workspaces Terminal
with:

[source,sh,role="copypaste"]
----
echo $GRAALVM_HOME
----

In this step, we will learn how to compile the application to a native executable and run the native image on local machine.

Compiling a native image takes longer than a regular JAR file (bytecode) compilation. However, this compilation time is only incurred once, as opposed to every time the application starts, which is the case with other approaches for building and executing JARs.

Let’s find out why Quarkus calls itself _SuperSonic Subatomic Subatomic Java_. Let’s build a sample app. In CodeReady Terminal, run this command:

[source,sh,role="copypaste"]
----
mkdir /tmp/hello && cd /tmp/hello && \
mvn io.quarkus:quarkus-maven-plugin:1.2.1.Final:create \
    -DprojectGroupId=org.acme \
    -DprojectArtifactId=getting-started \
    -DclassName="org.acme.quickstart.GreetingResource" \
    -Dpath="/hello"
----

This will create a simple Quarkus app in the */tmp/hello* directory.

Next, create a `native executable` with this command:

[source,sh,role="copypaste"]
----
mvn -f /tmp/hello/getting-started clean package -Pnative -DskipTests
----

This may take a minute or two to run. One of the benefits of Quarkus is amazingly fast startup time, at the expense of a longer build time to optimize and remove dead code, process annotations, etc. This is only incurred once, at build time rather than _every_ startup!

[NOTE]
====
Since we are on Linux in this environment, and the OS that will eventually run our application is also Linux, we can use our local OS to build the native Quarkus app. If you need to build native Linux binaries when on other OS’s like Windows or Mac OS X, you’ll need to have Docker installed and then use `mvn clean package -Pnative -Dnative-image.docker-build=true -DskipTests=true`.
====

image::payment-native-image-build.png[serverless, 700]

Since our environment here is Linux, you can just run it. In the CodeReady Workspaces Terminal, run:

[source,sh,role="copypaste"]
----
/tmp/hello/getting-started/target/*-runner
----

Notice the amazingly fast startup time:

[source,shell]
----
2020-02-22 03:11:28,173 INFO  [io.quarkus] (main) getting-started 1.0-SNAPSHOT (running on Quarkus 1.2.1.Final) started in 0.017s. Listening on: http://0.0.0.0:8080
2020-02-22 03:11:28,173 INFO  [io.quarkus] (main) Profile prod activated. 
2020-02-22 03:11:28,173 INFO  [io.quarkus] (main) Installed features: [cdi, resteasy]
----

That’s *17 milliseconds* to start up. The start-up time might be different in your environment.

And extremely low memory usage as reported by the Linux `ps` utility. While the app is running, run the following command in another Terminal:

[source,sh,role="copypaste"]
----
ps -o pid,rss,command -p $(pgrep -f runner)
----

You should see something like:

[source,shell]
----
    PID   RSS COMMAND
   2021 50392 /tmp/hello/getting-started/target/getting-started-1.0-SNAPSHOT-runner
----

This shows that our process is taking around `50 MB` of memory (https://en.wikipedia.org/wiki/Resident_set_size[Resident Set
Size,window=_blank], or RSS). Pretty compact!

[NOTE]
====
The RSS and memory usage of any app, including Quarkus, will vary depending your specific environment, and will rise as the application experiences load.
====

Make sure the app works. In a new CodeReady Workspaces Terminal run:

[source,sh,role="copypaste"]
----
curl -i http://localhost:8080/hello
----

You should see the return:

[source,console]
----
HTTP/1.1 200 OK
Content-Length: 5
Content-Type: text/plain;charset=UTF-8

hello
----

*Congratuations!* You’ve now built a Java application as a native executable JAR and a Linux native binary. We’ll explore the benefits of native binaries later in when we start deploying to Kubernetes.

Be sure to terminate the running Quarkus development via kbd:[CTRL+C] (or kbd:[Command+C] on Mac OS).

=== 2. Delete old payment service

_OpenShift Serverless_ builds on Knative Serving to support deploying and serving of serverless applications and functions. _Serverless_ is easy to get started with and scales to support advanced scenarios.

The OpenShift Serverless provides middleware primitives that enable:

* Rapid deployment of serverless containers
* Automatic scaling up and down to zero
* Routing and network programming for Istio components
* Point-in-time snapshots of deployed code and configurations

In the lab, _OpenShift Serverless Operator_ is already installed on your OpenShift 4 cluster but if you want to install it on your
own OpenShift cluster, follow https://docs.openshift.com/container-platform/latest/serverless/installing-openshift-serverless.html[Installing OpenShift Serverless,window=_blank].

First, we need to delete existing `BuildConfig` as it is based an excutable Jar that we deployed it in the previous lab.

[source,sh,role="copypaste"]
----
oc delete bc/payment
----

We also will delete our existing payment _deployment_ and _route_ since Knative will handle deploying the payment service and routing traffic to its managed pod when needed. Delete the existing payment deployment and its associated route and service with:

[source,sh,role="copypaste"]
----
oc delete dc/payment route/payment svc/payment
----

=== 3. Enable Knative Eventing integration with Apache Kafka Event

_Knative Eventing_ is a system that is designed to address a common need for cloud native development and provides composable primitives to enable `late-binding` event sources and event consumers with below goals:

* Services are loosely coupled during development and deployed independently.
* Producer can generate events before a consumer is listening, and a consumer can express an interest in an event or class of events that is not yet being produced.
* Services can be connected to create new applications without modifying producer or consumer, and with the ability to select a specific subset of events from a particular producer.

The _Apache Kafka Event source_ enables Knative Eventing integration with Apache Kafka. When a message is produced to Apache Kafka, the Event Source will consume the produced message and post that message to the corresponding event sink.

Remove direct Knative integration code. Currently our Payment service directly binds to Kafka to listen for events. Now that we have Knative eventing integration, we no longer need this code. Open the `PaymentResource.java` file (in `payment-service/src/main/java/com/redhat/cloudnative` directory).

Comment out the `onMessage()` method via kbd:[CTRL+/] (or kbd:[Command+/] on Mac OS):

[source,java]
----
//    @Incoming("orders")
//    public CompletionStage<Void> onMessage(KafkaMessage<String, String> message)
//            throws IOException {
//
//        log.info("Kafka message with value = {} arrived", message.getPayload());
//        handleCloudEvent(message.getPayload());
//        return message.ack();
//    }
----

And delete the configuration for the incoming stream. In `application.properties`, comment out the following lines for the _Incoming_ stream via kbd:[CTRL+/] (or kbd:[Command+/] on Mac OS):

[source,none]
----
# Incoming stream (unneeded when using Knative events)
; mp.messaging.incoming.orders.connector=smallrye-kafka
; mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
; mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
; mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
; mp.messaging.incoming.orders.group.id=payment-order-service
; mp.messaging.incoming.orders.auto.offset.reset=earliest
; mp.messaging.incoming.orders.enable.auto.commit=true
; mp.messaging.incoming.orders.request.timeout.ms=30000
----

Rebuild and re-deploy new payment service via running the following maven plugin in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -Pnative -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

The _-Pnative_ argument selects the native maven profile which invokes the _Graal compiler_. We’ve deleted our old build configuration that took a JAR file. We need a new build configuration that can take our new native
compiled Quarkus app. Create a new build config with this command:

[source,sh,role="copypaste"]
----
oc new-build quay.io/quarkus/ubi-quarkus-native-binary-s2i:20.0.0 --binary --name=payment -l app=payment
----

Start and watch the build, which will take about 3-4 minutes to complete:

[source,sh,role="copypaste"]
----
oc start-build payment --from-file=$CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/target/payment-1.0-SNAPSHOT-runner --follow
----

This step will combine the native binary with a base OS image, create a new container image, and push it to an internal image registry.

Create a `Knative Service` in the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View] and click on `+` icon on the right top corner:

image::plus-icon.png[serverless, 500]

Copy the following _Service_ in `YAML` editor then click on *Create*:

[source,sh,role="copypaste"]
----
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: payment
spec:
  template:
    metadata:
      name: payment-v1
      annotations:
        sidecar.istio.io/inject: "false"
    spec:
      containers:
      - image: image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnativeapps/payment:latest
----

After successful creation of the service we should see a *Knative Service*(_KSVC_) and *Revision*(_REV_) in the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View]:

image::kservice-up.png[serverless, 700]

In the lab environment, _OpenShift Serverless_ will automatically scale services down to zero instances when the service(i.e. payment) has no request after *30 seconds* which means the payment service pod will unavailable in 30 seconds. Visit again the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View, window=_blank]. Ensure there's no *blue circle* in the payment service!

image::kservice-down.png[serverless, 700]

If you send traffic to this endpoint it will trigger the autoscaler to scale the app up. Click on http://payment.{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN }}[Open URL] to _trigger_ the payment service. This will send some dummy data to the `payment` service, but more importantly it triggered knative to spin up the pod again automatically, and will shut it down 30 seconds later.

image::payment-serving-magic.png[serverless, 700]

*Congratulations!* You’ve now deployed the payment service as a Quarkus native image, served with _OpenShift Serverless_, quicker than traditional Java applications. This is not the end of Serverless capabilites so we will now see how the payment service will scale up _magically_ in the following exercises.

Let's move on to create *KafkaSource* to enable *Knative Eventing*. In this lab, _Knative Eventing_ is already installed via the _Knative Eventing Operator_ in OpenShift 4 cluster.

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View, window=_blank], click on `+` icon on the right top corner.

image::plus-icon.png[serverless, 500]

Copy the following `KafkaSource` in `YAML` editor then click on *Create*:

[source,sh,role="copypaste"]
----
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: KafkaSource
metadata:
  name: kafka-source
spec:
  consumerGroup: payment-consumer-group
  bootstrapServers: my-cluster-kafka-bootstrap:9092
  topics: orders
  sink:
    apiVersion: serving.knative.dev/v1alpha1
    kind: Service
    name: payment
----

You can see a new connection between Kafka and our *payments* service:

image::kafka-event-source-link.png[serverless, 700]

Create a *NetworkPolicy* to access normal, non-istioified sidecar applications(i.e. inventory). Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View, window=_blank], click on `+` icon on the right top corner. Copy the following `NetworkPolicy` in `YAML` editor then click on *Create*:

[source,sh,role="copypaste"]
----
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: knative-serving-allow-all
spec:
  egress:
  - {}
  ingress:
  - {}
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
----

*Great job!* Let’s make sure if the payment service works properly with _Serverless_ features via Coolstore Web UI.

=== 4. End to End Functional Testing

Before getting started, we need to make sure if _payment service_ is scaled down to _zero_ again in {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View, window=_blank]:

image::payment-down-again.png[serverless, 1000]

Let’s go shopping! Access the http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store, window=_blank]!

Add some cool items to your shopping cart in the following shopping scenarios:

[arabic]
. Add a _Forge Laptop Sticker_ to your cart by click on *Add to Cart*. You will see the `Success! Added!` message under the top menu.

image::add-to-cart-serverless.png[serverless, 1000]

[arabic, start=2]
. Go to the *Your Shopping Cart* tab and click on the *Checkout* button . Input the credit card information. The Card Info should be 16 digits and begin with the digit `4`. For example `4123987754646678`.

image::checkout-serverless.png[serverless, 1000]

[arabic, start=3]
. Input your Credit Card information to pay for the items:

image::input-cc-info-serverless.png[serverless, 1000]

[arabic, start=4]
. Let’s find out how _Kafka Event_ enables _Knative Eventing_. Go back to {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View, window=_blank] then confirm if *payment service* is up automatically. It’s `MAGIC!!`

image::payment-serving-magic.png[serverless, 500]

[arabic, start=5]
. Confirm the _Payment Status_ of the your shopping items in the *All Orders* tab. It should be `Processing`.

image::payment-processing-serverless.png[serverless, 1000]


[arabic, start=5]
. After a few moments, reload the *All Orders* page to confirm that the Payment Status changed to `COMPLETED` or `FAILED`.

[NOTE]
====
If the status is still *Processing*, the order service is processing incoming Kafka messages and store them in MongoDB. Please reload the page a few times more.
====

image::payment-completedorfailed-serverless.png[serverless, 1000]

This is the same result as before, but using Knative eventing to make a more powerful event-driven system that can scale with demand.

=== 5. Creating Cloud-Native CI/CD Pipelines using Tekton

There’re lots of open source CI/CD tools to build, test, deploy, and manage cloud-native applications/microservices: from on-premise to private, public, and hybrid cloud. Each tool provides different features to integrate with existing
platforms/systems. This sometimes makes it more complex for DevOps teams to be able to create the CI/CD pipelines and maintain them on Kubernetes clusters. The *Cloud-Native CI/CD Pipeline* should be defined and executed in the Kubernetes native way. For example, the pipeline can be specified as Kubernetes resources using YAML format.

*OpenShift Pipelines* is a Kubernetes-style CI/CD solution based on _Tekton_. It builds on the Tekton building blocks and provides a CI/CD experience through tight integration with OpenShift and Red Hat developer tools. OpenShift Pipelines is designed to run each step of the CI/CD pipeline in its own container, allowing each step to scale independently to meet the demands of the pipeline with the below features:

* Use standard Tekton CRDs to define pipelines that run as containers and scale on-demand.
* Full control over team’s delivery pipelines, plugins and access control with no central CI/CD server to manage.
* A streamlined user experience through the OpenShift Console developer perspective, CLIs, and IDEs.

image::pipeline-features.png[pipeline, 800]

[NOTE]
====
OpenShift Pipelines project is as a Developer Preview release. Developer Preview releases have features and functionality that might not be fully tested. Customers are encouraged to use and provide feedback on Developer Preview releases. Red Hat does not commit to fixing any reported issues and the provided features may not be available in future releases.
====

In the lab, OpenShift Pipelines is already installed on OpenShift 4 cluster but if you want to install OpenShift Pipelines on your own OpenShift cluster, OpenShift Pipelines is provided as an add-on on top of OpenShift that can be installed via an operator available in the OpenShift OperatorHub.

In order to define a pipeline, you need to create _custom resources_ as listed below:

* *Task*: a reusable, loosely coupled number of steps that perform a specific task (e.g. building a container image)
* *Pipeline*: the definition of the pipeline and the tasks that it should perform
* *PipelineResource*: inputs (e.g. git repository) and outputs (e.g. image registry) to and out of a pipeline or task
* *TaskRun*: the execution and result (i.e. success or failure) of running an instance of task
* *PipelineRun*: the execution and result (i.e. success or failure) of running a pipeline

image::tekton-arch.png[severless, 800]

For further details on pipeline concepts, refer to the https://github.com/tektoncd/pipeline/tree/master/docs#learn-more[Tekton documentation,window=_blank] that provides an excellent guide for understanding various parameters and attributes available for defining pipelines.

In this lab, we will walk you through pipeline concepts and how to create and run a CI/CD pipeline for building and deploying microservices on OpenShift Serverless platform.

Let's deploy a https://github.com/spring-projects/spring-petclinic[Spring PetClinic,window=_blank] microserivces on Spring Boot framework to `{{ USER_ID }}-cloudnative-pipeline` project.

Create the Kubernetes objects for deploying the _PetClinic_ app on OpenShift. The deployment will not complete since there are no container images built for the PetClinic application yet. That you will do in the following sections through a CI/CD pipeline.

Open *petclinic.yaml* in _knative/pipeline_ of *payment-service* project and copy the following code into the YAML file:

[source,yaml,role="copypaste"]
----
---
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  labels:
    app: spring-petclinic
  name: spring-petclinic
---
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  labels:
    app: spring-petclinic
  name: spring-petclinic
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    app: spring-petclinic
    deploymentconfig: spring-petclinic
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      labels:
        app: spring-petclinic
        deploymentconfig: spring-petclinic
    spec:
      containers:
      - image: spring-petclinic:latest
        imagePullPolicy: Always
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 45
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: spring-petclinic
        ports:
        - containerPort: 8080
          protocol: TCP
        - containerPort: 8443
          protocol: TCP
        - containerPort: 8778
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 45
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
  triggers:
  - imageChangeParams:
      containerNames:
      - spring-petclinic
      from:
        kind: ImageStreamTag
        name: spring-petclinic:latest
        namespace: {{ USER_ID }}-cloudnative-pipeline
    type: ImageChange
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: spring-petclinic
  name: spring-petclinic
spec:
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: 8443-tcp
    port: 8443
    protocol: TCP
    targetPort: 8443
  - name: 8778-tcp
    port: 8778
    protocol: TCP
    targetPort: 8778
  selector:
    app: spring-petclinic
    deploymentconfig: spring-petclinic
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    app: spring-petclinic
  name: spring-petclinic
spec:
  port:
    targetPort: 8080-tcp
  to:
    kind: Service
    name: spring-petclinic
    weight: 100
----

Then create the object in Kubernetes:

[source,sh,role="copypaste"]
----
oc project {{ USER_ID }}-cloudnative-pipeline
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/petclinic.yaml
----

Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnative-pipeline[Topology View, window=_blank] for the pipeline, and ensure you *don't* get the blue circles yet!

image::petclinic-deployed-1.png[serverless, 500]

*Tasks* consist of a number of steps that are executed sequentially. Each _task_ is executed in a separate container within the same pod. They can also have inputs and outputs in order to interact with other tasks in the pipeline.

When a _task_ starts running, it starts a pod and runs each *step* sequentially in a separate container on the same pod. This task happens to have a single step, but tasks can have multiple steps, and, since they run within the same pod, they have access to the same volumes in order to cache files, access configmaps, secrets, etc. `Tasks` can also receive inputs (e.g., a git repository) and outputs (e.g., an image in a registry) in order to interact with each other.

[NOTE]
====
Only the requirement for a git repository is declared on the task and not a specific git repository to be used. That allows _tasks_ to be reusable for multiple pipelines and purposes. You can find more examples of reusable _tasks_ in the https://github.com/tektoncd/catalog[Tekton Catalog, window=_blank] and
https://github.com/openshift/pipelines-catalog[OpenShift Catalog, window=_blank] repositories.
====

Install the `openshift-client` and `s2i-java` tasks from the catalog repository using _oc_ or _kubectl_, which you will need for creating a pipeline in the next section. Create the following Tekton tasks which will be used in the *Pipelines*:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/openshift-client-task.yaml

oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/s2i-java-8-task.yaml
----

Let’s confirm if the *tasks* are installed properly using https://github.com/tektoncd/cli/releases[Tekton CLI,window=_blank] that already installed in CodeReady Workspaces:

[source,sh,role="copypaste"]
----
tkn task list
----

You will see the following 2 tasks:

[source,sh]
----
NAME               AGE
openshift-client   27 seconds ago
s2i-java-8         27 seconds ago
----

A pipeline defines a number of tasks that should be executed and how they interact with each other via their inputs and outputs.

In this lab, we will create a pipeline that takes the source code of PetClinic application from GitHub and then builds and deploys it on OpenShift using
https://docs.openshift.com/container-platform/latest/builds/understanding-image-builds.html#build-strategy-s2i_understanding-image-builds[Source-to-Image(S2I),window=_blank].

image::pipeline-diagram.png[serverless, 700]

This pipeline performs the following:

* Clones the source code of the application from a Git repository (_app-git_ resource)
* Builds the container image using the s2i-java-8 task that generates a Dockerfile for the application and uses https://buildah.io/[Buildah,window=_blank] to build the image 
* The application image is pushed to an image registry (_app-image_ resource)
* The new application image is deployed on OpenShift using the _openshift-cli_

[NOTE]
====
You might have noticed that there are no references to the PetClinic Git repository and its image in the registry. That’s because _Pipelines_ in Tekton are designed to be generic and re-usable across environments and stages through the application’s lifecycle.
====

Pipelines abstract away the specifics of the Git source repository and image to be produced as *resources*. When triggering a pipeline, you can provide different Git repositories and image registries to be used during pipeline execution. Be patient! You will do that in a little bit in the next section.

The execution order of _tasks_ is determined by dependencies that are defined between the tasks via *inputs* and *outputs* as well as explicit orders that are defined via *runAfter*.

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnative-pipeline[Topology View, window=_blank], and click on *Create Pipeline*:

image::console-import-yaml-1.png[serverless, 800]

Replace the following YAML into the textfield, and click on *Create*:

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: Pipeline
metadata:
  name: petclinic-deploy-pipeline
spec:
  resources:
  - name: app-git
    type: git
  - name: app-image
    type: image
  tasks:
  - name: build
    taskRef:
      name: s2i-java-8
    params:
      - name: TLSVERIFY
        value: "false"
    resources:
      inputs:
      - name: source
        resource: app-git
      outputs:
      - name: image
        resource: app-image
  - name: deploy
    taskRef:
      name: openshift-client
    runAfter:
      - build
    params:
    - name: ARGS
      value:
        - rollout
        - latest
        - spring-petclinic
----

You will see the pipeline you have created:

image::console-import-yaml-2.png[serverless, 800]

Now that the pipeline is created, you can trigger it to execute the tasks specified in the pipeline. First, you should create a number of *PipelineResources* that contain the specifics of the Git repository and image registry to be used in the pipeline during execution. Expectedly, these are also reusable across multiple pipelines.

The following *PipelineResource* defines the Git repository and reference for the PetClinic application. Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnative-pipeline[Topology View, window=_blank], and click on `+` icon on the right top corner. Copy the following `petclinic-git` pipelineResource in `YAML` editor then click on *Create*:

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: petclinic-git
spec:
  type: git
  params:
  - name: url
    value: https://github.com/spring-projects/spring-petclinic
----

And the following defines the OpenShift internal registry for the PetClinic image to be pushed to. Create the following pipeline resources via the {{ CONSOLE_URL }}[OpenShift web console,window=_blank] via clicking again on `+` icon on the right top corner. Copy the following `petclinic-image` pipelineResource in `YAML` editor then click on *Create*:

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: petclinic-image
spec:
  type: image
  params:
  - name: url
    value: image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/spring-petclinic
----

Execute the following _Tekton CLI_ in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
tkn resource ls
----

You can see the list of resources created:

[source,shell]
----
NAME              TYPE    DETAILS
petclinic-git     git     url: https://github.com/spring-projects/spring-petclinic
petclinic-image   image   url: image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/spring-petclinic
----

A *PipelineRun* is how you can start a pipeline and tie it to the Git and image resources that should be used for this specific invocation. Let's go to {{ CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev~v1alpha1~Pipeline[OpenShift Pipelines, window=_blank] and click on *Start*:

image::pipeline-start.png[serverless, 800]

Then click on *Start* in the resources popup:

image::pipeline-start-popup.png[serverless, 700]

As soon as you started the *petclinic-deploy-pipeline*, a pipelinerun is instantiated and pods are created to execute the tasks that are defined in the pipeline. After a few minutes, the pipeline should finish successfully.

image::pipeline-complete.png[serverless, 800]


Looking back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnative-pipeline[Topology View, window=_blank], and click on *Open URL*. You should see that the PetClinic image is successfully built and deployed.

image::petclinic-openurl.png[serverless, 800]

=== Summary

In this module, we learned how to develop cloud-native applications using multiple Java runtimes (Quarkus and Spring Boot), Javascript (Node.js) and different datasources (i.e. PostgreSQL, MongoDB) to handle a variety of business use cases which implement real-time _request/response_ communication using REST APIs, high performing cacheable services using _JBoss Data Grid_, event-driven/reactive shopping cart service using Apache Kafka in _Red Hat AMQ Streams_, and in the end, we treated the payment service as a *Serverless* application using _OpenShift Serverless_ with _Knative Eventing_.

*Red Hat Runtimes* enables enterprise developers to design the advanced cloud-native architecture and develop, build, deploy the cloud-native application on hybrid cloud on the *Red Hat OpenShift Container Platform*. *Congratulations!*

==== Additional Resources:

* https://learn.openshift.com/middleware/courses/middleware-quarkus/[Quarkus Tutorials Right in Your Browser,window=_blank]
* https://developers.redhat.com/articles/quarkus-quick-start-guide-kubernetes-native-java-stack/[Quarkus Quickstart Guide]
* https://docs.openshift.com/container-platform/latest/serverless/serverless-getting-started.html[Getting started with OpenShift Serverless,window=_blank]
* https://www.openshift.com/learn/topics/pipelines[Cloud-native CI/CD on OpenShift,window=_blank]
* https://developers.redhat.com/topics/serverless-architecture/[Serverless Architecture Articles,window=_blank]