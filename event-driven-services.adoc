= Lab2 - Creating Event-Driven/Reactive Services
:experimental:

Traditional microservice architectures are typically composed of many individual services with different functions. Each application service has many clients that need to communicate with the service for fetching data. It will become more complex to handle data streams because everything can be a stream of data such as end-user clicks, RESTful APIs, IoT devices generating data. And complexity rises when these services are running on hybrid or multi-cloud infrastructure.

In an event-driven architecture, we can treat data streams as *events* using reactive programming and distributed messaging. *Reactive programming* is an asynchronous programming paradigm concerned with data streams and the propagation of change. In the previous lab, we developed Inventory, Catalog, Shopping Cart and Order services with obvious interactions.

In this lab, we’ll change our shopping cart and order implementation and add a payment service as an Event-Driven/Reactive application in our cloud-native appliation architecture. These cloud-native applications will use AMQ Streams (based on Apache Kafka) as a messaging/streaming backbome. AMQ Streams makes it easy to run Apache Kafka on OpenShift with key features:

* Designed for horizontal scalability
* Message ordering guarantee at the partition level
* Message rewind/replay - _Long term_ storage allows the reconstruction of an application state by replaying the messages

=== Goals of this lab

The goal is to develop advanced cloud-native applications on *Red Hat Runtimes* and deploy them on *OpenShift 4* including *AMQ Streams* for distributed messaing capabilities. After this lab, you should end up with something like:

image::lab2-goal.png[goal, 700]

Scalability is one of the flagship features of Apache Kafka. It is achieved by partitioning the data and distributing them across multiple brokers. Such data sharding also has a big impact on how clients connect and use the broker. This is especially visible when Kafka is running within a platform like Kubernetes but is accessed from outside of that platform.

https://strimzi.io/[Strimzi] is an open source project that provides container images and operators for running https://developers.redhat.com/videos/youtube/CZhOJ_ysIiI/[Apache kafka].

In this lab, we will use productized and supported versions of the Strimzi and Apache Kafka projects through https://www.redhat.com/en/technologies/jboss-middleware/amq?extIdCarryOver=true&sc_cid=701f2000001OH7TAAW[Red Hat AMQ^].

=== 1. Create a Kafka Cluster and Topics

AMQ Streams is already installed using the following _Operators_ so you don’t need to install it in this lab:

* *Kafka Operator* - Responsible for deploying and managing Apache Kafka clusters within an OpenShift cluster.
* *Topic Operator* - Responsible for managing Kafka topics within a Kafka cluster running within an OpenShift cluster.
* *User Operator* - Responsible for managing Kafka users within a Kafka cluster running within an OpenShift cluster.

The basic architecture of operators in AMQ is seen below:

image::kafka-operators-arch.png[amqstreams, 700]

Let's create a **Kafka cluster**. Click *+Add* on the left, on the _From Catalog_ box on the project overview:

image::kafka-catalog.png[kafka, 700]

Type in `kafka` in the search box, and click on the *Kafka*:

image::kafka-create.png[kafka, 700]

Click on *Create* and you will enter YAML editor that defines a *Kafka* Cluster. Keep the all values as-is then click on *Create* on the botton:

image::kafka-create-detail.png[kafka, 700]

Next, we will create Kafka _Topic_. Move to *Kafka Topic* tab then click on *Create Kafka Topic* on the botton:

image::kafka-topic-catalog.png[kafka, 700]

You will enter YAML editor that defines a *KafkaTopic* object. Change the name to `orders` as shown then click on *Create* on the bottom:

image::kafka-topic-orders-create.png[kafka, 700]

Create another topic using the same process as above, but called `payments`:

image::kafka-another-topic-create.png[kafka, 700]

Change the name to `payments` then click on *Create* on the bottom.

image::kafka-topic-payments-create.png[kafka, 700]

*Well done!* You now have a running Kafka cluster with two Kafka Topics called `payments` and `orders`.

image::kafka-topics-created.png[kafka, 700]

Label them for easy visualizing using the AMQ Streams icon:

[source,sh,role="copypaste"]
----
oc label kafka/my-cluster app.openshift.io/runtime=amq --overwrite && \
oc label kafkatopic/payments app.openshift.io/runtime=amq --overwrite && \
oc label kafkatopic/orders app.openshift.io/runtime=amq --overwrite
----


=== 2. Develop and Deploy Payment Service

Our _Payment Service_ will offer online services for accepting electronic payments by a variety of payment methods including credit card or bank-based payments when orders are checked out in shopping cart. It doesn’t really do anything but will represent a payment microservice that will *process* online shopping orders as they are posted to our services.

In CodeReady Workspaces, expand *payment-service* directory.

image::codeready-workspace-payment-project.png[catalog, 700]

In this step, we will learn how our Quarkus-based payment service can use Kafka to receive order events and _react_ with payment events.

Let's add Maven Dependencies using Quarkus Kafka extensions in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="kafka" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

You should see in the output:

[source,console]
----
✅ Adding extension io.quarkus:quarkus-smallrye-reactive-messaging-kafka
----

This command imports the Kafka extensions for Quarkus applications and provides all the necessary capabilities to integrate with Kafka clusters. It adds this to `pom.xml` for you:

[source,xml]
----
    ...
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
      <version>${quarkus.version}</version>
    </dependency>
    ...
----

Let’s start by adding injecting configuration using *@ConfigProperty* and a *Producer* field which will be used to send messages. We’ll also add a `log` field so we can see debug messages later on.

Add this code to the `PaymentResource.java` file (in the `src/main/java/com/redhat/cloudnative` directory) at the `// TODO: Add Messaging ConfigProperty here` marker:

[source,java,role="copypaste"]
----
    @ConfigProperty(name = "mp.messaging.outgoing.payments.bootstrap.servers")
    public String bootstrapServers;

    @ConfigProperty(name = "mp.messaging.outgoing.payments.topic")
    public String paymentsTopic;

    @ConfigProperty(name = "mp.messaging.outgoing.payments.value.serializer")
    public String paymentsTopicValueSerializer;

    @ConfigProperty(name = "mp.messaging.outgoing.payments.key.serializer")
    public String paymentsTopicKeySerializer;

    private Producer<String, String> producer;

    public static final Logger log = LoggerFactory.getLogger(PaymentResource.class);
----

Next, we need a method to handle incoming events, which in this lab will be coming directly from Kafka, but later will come through as HTTP POST events.

Add this code at the `// TODO: Add handleCloudEvent method here` marker:

[source,java,role="copypaste"]
----
    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.TEXT_PLAIN)
    public void handleCloudEvent(String cloudEventJson) {
        String orderId = "unknown";
        String paymentId = "" + ((int)(Math.floor(Math.random() * 100000)));

        try {
            log.info("received event: " + cloudEventJson);
            JsonObject event = new JsonObject(cloudEventJson);
            orderId = event.getString("orderId");
            String total = event.getString("total");
            JsonObject ccDetails = event.getJsonObject("creditCard");
            String name = event.getString("name");

            // fake processing time
            Thread.sleep(5000); // <1>
            if (!ccDetails.getString("number").startsWith("4")) {
                 fail(orderId, paymentId, "Invalid Credit Card: " + ccDetails.getString("number"));
            }
             pass(orderId, paymentId, "Payment of " + total + " succeeded for " + name + " CC details: " + ccDetails.toString());
        } catch (Exception ex) {
             fail(orderId, paymentId, "Unknown error: " + ex.getMessage() + " for payment: " + cloudEventJson);
        }
    }
----
<1> This will simulate a 5 second credit card processing time

Now we need to implement the `pass()` and `fail()` methods referenced above. These methods will send messages to Kafka using our _producer_ field.

Add the following code to the `// TODO: Add pass method here` marker:

[source,java,role="copypaste"]
----
    private void pass(String orderId, String paymentId, String remarks) {

        JsonObject payload = new JsonObject();
        payload.put("orderId", orderId);
        payload.put("paymentId", paymentId);
        payload.put("remarks", remarks);
        payload.put("status", "COMPLETED");
        log.info("Sending payment success: " + payload.toString());
        producer.send(new ProducerRecord<String, String>(paymentsTopic, payload.toString()));
    }
----

Add this code to the `// TODO: Add fail method here` marker:

[source,java,role="copypaste"]
----
    private void fail(String orderId, String paymentId, String remarks) {
        JsonObject payload = new JsonObject();
        payload.put("orderId", orderId);
        payload.put("paymentId", paymentId);
        payload.put("remarks", remarks);
        payload.put("status", "FAILED");
        log.info("Sending payment failure: " + payload.toString());
        producer.send(new ProducerRecord<String, String>(paymentsTopic, payload.toString()));
    }
----

Next, add a method that will receive events from Kafka. We will use the MicroProfile reactive messaging API `@Incoming` annotation to do this.

Add this code to the `// TODO: Add consumer method here` marker:

[source,java,role="copypaste"]
----
    @Incoming("orders")
    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
            throws IOException {

        log.info("Kafka message with value = {} arrived", message.getPayload());
        handleCloudEvent(message.getPayload());
        return message.ack();
    }
----

And finally, we need a method to initialize the Kafka producer (the consumer will be initialized automatically via Quarkus Kafka extension). We will use the Quarkus `StartupEvent` Lifecycle listener API, with the `@Observes` annotation to mark this method as one that should run when the app starts:

Add this code to the `// TODO: Add init method here` marker:

[source,java,role="copypaste"]
----
    public void init(@Observes StartupEvent ev) {
        Properties props = new Properties();

        props.put("bootstrap.servers", bootstrapServers);
        props.put("value.serializer", paymentsTopicValueSerializer);
        props.put("key.serializer", paymentsTopicKeySerializer);
        producer = new KafkaProducer<String, String>(props);
    }
----

This method will consume Kafka streams from the `orders` topic and call our `handleCloudEvent()` method. Later on we’ll delete this method and use Knative Events to handle the incoming stream. But for now we’ll use this method to listen to the topic.

Quarkus and its extensions are configured by an `application.properties` file. Open this file (it is in the `src/main/resources` directory).

Add these values to the file:

[source,properties,role="copypaste"]
----
# Outgoing stream
mp.messaging.outgoing.payments.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.outgoing.payments.connector=smallrye-kafka
mp.messaging.outgoing.payments.topic=payments
mp.messaging.outgoing.payments.value.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.payments.key.serializer=org.apache.kafka.common.serialization.StringSerializer

# Incoming stream (unneeded when using Knative events)
mp.messaging.incoming.orders.connector=smallrye-kafka
mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.incoming.orders.group.id=payment-order-service
mp.messaging.incoming.orders.auto.offset.reset=earliest
mp.messaging.incoming.orders.enable.auto.commit=true
mp.messaging.incoming.orders.request.timeout.ms=30000
----

Build and deploy the project using the following command, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

Create a build configuration for your application using OpenJDK base container image in OpenShift:

[source,sh,role="copypaste"]
----
oc new-build registry.access.redhat.com/redhat-openjdk-18/openjdk18-openshift:1.5 --binary --name=payment -l app=payment
----

Start and watch the build, which will take about minutes to complete:

[source,sh,role="copypaste"]
----
oc start-build payment --from-file=$CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/target/payment-1.0-SNAPSHOT-runner.jar --follow
----

Deploy it as an OpenShift application after the build is done:

[source,sh,role="copypaste"]
----
oc new-app payment -l app.openshift.io/runtime=java && oc expose svc/payment && \
oc label dc/payment app.kubernetes.io/part-of=payment --overwrite && \
oc annotate dc/payment app.openshift.io/connects-to=my-cluster-kafka --overwrite && \
oc annotate dc/payment app.openshift.io/vcs-uri=https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs.git --overwrite && \
oc annotate dc/payment app.openshift.io/vcs-ref=ocp-4.3 --overwrite && \
oc annotate statefulset my-cluster-kafka app.openshift.io/connects-to=my-cluster-zookeeper --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the orders. Ensure you get the blue circles!

image::payment-topology.png[order, 700]

In order to test the payment application, click on the `my-cluster-kafka-0` pod:

image::my-cluster-kafka-0.png[payment, 700]

We will watch the Kafka topic via a CLI to confirm the messages are being sent/received in Kafka. Click on the *Terminal* tab in OpenShift (not in CodeReady!) then execute the following command:

[source,sh,role="copypaste"]
----
bin/kafka-console-consumer.sh --topic payments --bootstrap-server localhost:9092
----

image::kafka-console-consumer.png[payment, 900]

Keep this tab open to act as a debugger for Kafka messages.

Now, back in *CodeReady Workspaces*, open a new Terminal.

Let’s produce a new topic message using `curl`:

First, fetch the URL of our new payment service and store it in an environment variable:

[source,sh,role="copypaste"]
----
export URL=http://$(oc get route -n {{ USER_ID}}-cloudnativeapps payment -o jsonpath={% raw %}"{.spec.host}"{% endraw %})
----

Then execute this to HTTP POST a message to our payment service with an example order:

[source,sh,role="copypaste"]
----
curl -i -H 'Content-Type: application/json' -X POST -d'{"orderId": "12321","total": "232.23", "creditCard": {"number": "4232454678667866","expiration": "04/22","nameOnCard": "Jane G Doe"}, "billingAddress": "123 Anystreet, Pueblo, CO 32213", "name": "Jane Doe"}' $URL
----

This will return in 5 seconds (our fake credit card processing time).

The payment service will recieve this _order_ and produce a _payment_ result on the Kafka _payment_ topic. You will see the
following result in `Pod Terminal`:

[source,shell]
----
{"orderId":"12321","paymentId":"25658","remarks":"Payment of 232.23 succeeded for Jane Doe CC details: {\"number\":\"4232454678667866\",\"expiration\":\"04/22\",\"nameOnCard\":\"Jane G Doe\"}","status":"COMPLETED"}
----


This shows that the `order-service` can successfully process orders in the backend and send them to the payment processor.

Before moving to the next step, stop the Kafka consumer console via kbd:[CTRL+C]:

image::kafka-console-consumer-stop.png[payment, 900]

=== 3. Adding Kafka Client to Cart Service

By now we have added several microservices to operate on our retail shopping data. Quite often, other services or functions would need the data we are working with. e.g. once a user checks out, there are other services like an _Order Service_ and our _Payment Service_ that will need this information, and would most likely want to process further. So we will integrate our Cart service
with Kafka so that it can send an order message when a shopper checks out.

Add Maven Dependencies using Quarkus *Kafka* Extensions:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="kafka" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service
----

You should see in the output:

[source,console]
----
✅ Adding extension io.quarkus:quarkus-smallrye-reactive-messaging-kafka
----

This will add the Kafka extension and APIs to our Cart service app.

Like our Payment service, add this code to the `// TODO: Add annotation of orders messaging configuration here` marker inside the `CartResource` class inside the `com.redhat.cloudnative` package in the `cart-service` subdirectory of your workspace:

[source,java,role="copypaste"]
----
    @ConfigProperty(name = "mp.messaging.outgoing.orders.bootstrap.servers")
    public String bootstrapServers;

    @ConfigProperty(name = "mp.messaging.outgoing.orders.topic")
    public String ordersTopic;

    @ConfigProperty(name = "mp.messaging.outgoing.orders.value.serializer")
    public String ordersTopicValueSerializer;

    @ConfigProperty(name = "mp.messaging.outgoing.orders.key.serializer")
    public String ordersTopicKeySerializer;

    private Producer<String, String> producer;
----

Next, `un-comment` (or add if they are missing) the following `import` statements:

[source,java]
----
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
----

The `init()` method creates the Kafka configuration, and we have externalized this configuration and injected the variables as properties on the class. Replace the empty `init()` method with this code:

[source,java,role="copypaste"]
----
    public void init(@Observes StartupEvent ev) {
        Properties props = new Properties();

        props.put("bootstrap.servers", bootstrapServers);
        props.put("value.serializer", ordersTopicValueSerializer);
        props.put("key.serializer", ordersTopicKeySerializer);
        producer = new KafkaProducer<String, String>(props);
    }
----

The `sendOrder()` method is quite simple, it takes the Order POJO as a param and serializes that into JSON to send over the Kafka
topic. Replace the empty `sendOrder()` method with this code:

[source,java,role="copypaste"]
----
    private void sendOrder(Order order, String cartId) {
        order.setTotal(shoppingCartService.getShoppingCart(cartId).getCartTotal() + "");
        producer.send(new ProducerRecord<String, String>(ordersTopic, Json.encode(order)));
        log.info("Sent message: " + Json.encode(order));
    }
----

Now that we have those methods, Let's add a call to our `sendOrder()` method when we are checking out. Replace the code for `checkout()` with this code:

[source,java,role="copypaste"]
----
    @POST
    @Path("/checkout/{cartId}")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    @Operation(summary = "checkout")
    public ShoppingCart checkout(@PathParam("cartId") String cartId, Order order) {
        sendOrder(order, cartId);
        return shoppingCartService.checkout(cartId);
    }
----

Almost there! Next let’s add the configuration to our `application.properties` file (in the `src/main/resources` of the `cart-service` project):

[source,none,role="copypaste"]
----
mp.messaging.outgoing.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.outgoing.orders.connector=smallrye-kafka
mp.messaging.outgoing.orders.topic=orders
mp.messaging.outgoing.orders.value.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.orders.key.serializer=org.apache.kafka.common.serialization.StringSerializer
----

Re-package the _cart service_ using the following command, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service
----

Rebuild a container image based the cart artifact that we just packaged, which will take about minutes to complete:

[source,sh,role="copypaste"]
----
oc start-build cart --from-file $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service/target/*-runner.jar --follow && \
oc annotate dc/cart app.openshift.io/connects-to=my-cluster-kafka,datagrid-service --overwrite
----

The cart service will be redeployed automatically via https://docs.openshift.com/container-platform/4.1/applications/deployments/managing-deployment-processes.html#deployments-triggers_deployment-operations[OpenShift Deployment triggers^] after it completes to build.

=== 4. Adding Kafka Client to Order Service

Like the *payments* service, our *order* service will listen for orders being placed, but will not process payments - instead the order service will merely record the orders and their states for eventual display in the UI. Let’s add this capability to the order service.

Add Maven Dependencies using Quarkus *Kafka* Extensions:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="kafka" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service
----

You should see in the output:

[source,console]
----
✅ Adding extension io.quarkus:quarkus-smallrye-reactive-messaging-kafka
----

This command generates a Maven project, importing the Kafka extensions for Quarkus applications and provides all the necessary capabilities to integrate with Kafka and subscribe to the _payments_ and _orders_ topic.

To process the incoming and outgoing messages, Create a new Java class named `KafkaOrders.java` in the `src/main/java/com/redhat/cloudnative` directory to consume messages from the Kafka _orders_ and _payments_ topic. Copy the following entire code into _KafkaOrders.java_.

[source,java,role="copypaste"]
----
package com.redhat.cloudnative;

import io.smallrye.reactive.messaging.kafka.KafkaRecord;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.enterprise.context.ApplicationScoped;

import java.io.IOException;
import java.util.concurrent.CompletionStage;

import javax.inject.Inject;
import io.vertx.core.json.JsonObject;

@ApplicationScoped
public class KafkaOrders {

    private static final Logger LOG = LoggerFactory.getLogger(KafkaOrders.class);

    @Inject
    OrderService orderService;

    @Incoming("orders")
    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
            throws IOException {

        LOG.info("Kafka order message with value = {} arrived", message.getPayload());

        JsonObject orders = new JsonObject(message.getPayload());
        Order order = new Order();
        order.setOrderId(orders.getString("orderId"));
        order.setName(orders.getString("name"));
        order.setTotal(orders.getString("total"));
        order.setCcNumber(orders.getJsonObject("creditCard").getString("number"));
        order.setCcExp(orders.getJsonObject("creditCard").getString("expiration"));
        order.setBillingAddress(orders.getString("billingAddress"));
        order.setStatus("PROCESSING");
        orderService.add(order);

        return message.ack();
    }

    @Incoming("payments")
    public CompletionStage<Void> onMessagePayments(KafkaRecord<String, String> message)
            throws IOException {

        LOG.info("Kafka payment message with value = {} arrived", message.getPayload());

        JsonObject payments = new JsonObject(message.getPayload());
        orderService.updateStatus(payments.getString("orderId"), payments.getString("status"));

        return message.ack();
    }

}
----

Almost there; Next Let's add the configuration to our `src/main/resources/application.properties` file in the _order-service_ project:

[source,none,role="copypaste"]
----
# Incoming payment topic messages
mp.messaging.incoming.payments.connector=smallrye-kafka
mp.messaging.incoming.payments.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.payments.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.payments.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.incoming.payments.group.id=order-service
mp.messaging.incoming.payments.auto.offset.reset=earliest
mp.messaging.incoming.payments.enable.auto.commit=true
mp.messaging.incoming.payments.request.timeout.ms=30000

# Enable CORS requests from browsers
quarkus.http.cors=true

# Incoming order topic messages
mp.messaging.incoming.orders.connector=smallrye-kafka
mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.incoming.orders.group.id=order-service
mp.messaging.incoming.orders.auto.offset.reset=earliest
mp.messaging.incoming.orders.enable.auto.commit=true
mp.messaging.incoming.orders.request.timeout.ms=30000
----

Re-package the order service using the following command, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service
----

Rebuild a container image based the cart artifact that we just packaged, which will take about minutes to complete:

[source,sh,role="copypaste"]
----
oc start-build order --from-file=$CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service/target/order-1.0-SNAPSHOT-runner.jar --follow && \
oc annotate dc/order app.openshift.io/connects-to=my-cluster-kafka,order-database --overwrite
----

The order service will be redeployed automatically via https://docs.openshift.com/container-platform/4.1/applications/deployments/managing-deployment-processes.html#deployments-triggers_deployment-operations[OpenShift Deployment triggers^] after it completes to build.

Wait for all of the rebuilds and redeployments to complete, and you have all your blue circles!

Let’s confirm if the all services works correctly using `Kafka` messaging via coolstore GUI test.

=== 5. End to End Functional Testing

Let’s go shopping! Access the http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^]!

Add some cool items to your shopping cart in the following shopping scenarios:

[arabic]
. Add a _Red Hat Fedora_ to your cart by click on *Add to Cart*. You will see the `Success! Added!` message under the top menu.

image::add-to-cart.png[serverless, 1000]

[arabic, start=2]
. Go to the *Your Shopping Cart* tab and click on the *Checkout* button . Input the credit card information. The Card Info should be 16 digits and begin with the digit `4`. For example `4123987754646678`.

[NOTE]
====
Our fake credit card processor will look for credit card numbers that begin with `4`. Any credit card number that does not start with `4` will result in a `FAILED` processing. This is a good way to check the logic of our code!
====

image::checkout.png[serverless, 1000]

[arabic, start=3]
. Input your Credit Card information to pay for the items:

image::input-cc-info.png[serverless, 1000]

[arabic, start=4]
. Confirm the _Payment Status_ of the your shopping items in the *All Orders* tab. It should be `Processing`.

image::payment-processing.png[serverless, 1000]

[arabic, start=5]
. After a few moments, reload the *All Orders* page to confirm that the Payment Status changed to `COMPLETED` or `FAILED`.

[NOTE]
====
If the status is still `Processing`, the order service is processing incoming Kafka messages and storing them in MongoDB. Please reload the page a few times more.
====

image::payment-completedorfailed.png[serverless, 1000]

=== Summary

In this scenario we developed an _Event-Driven/Reactive_ cloud-native appliction to deal with data streams from the shopping cart service to the order service and payment service using _Apache Kafka_. We also used Quarkus and its _Kafka extension_ to integrate the app with Kafka. _AMQ Streams_, a fully supported Kafka solution from Red Hat, enables you to create Apache Kafka clusters very easily via the OpenShift developer catalog.

We now have message-driven microservices for implementing reactive systems, where all the components interact using asynchronous message passing for greater reliability, scalability, and faster time to market. Most importantly, Quarkus is perfectly suited to implement event-driven microservices and reactive systems. *Congratulations!*
