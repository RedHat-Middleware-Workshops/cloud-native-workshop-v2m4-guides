= Lab2 - Creating Event-Driven/Reactive Services
:experimental:
:imagesdir: images

Traditional microservice architectures are typically composed of many individual services with different functions. Each application service has many clients that need to communicate with the service for fetching data. It will become more complex to handle data streams because everything can be a stream of data such as end-user clicks, RESTful APIs, IoT devices generating data. And complexity rises when these services are running on hybrid or multi-cloud infrastructure.

In an event-driven architecture, we can treat data streams as *events* using reactive programming and distributed messaging. *Reactive programming* is an asynchronous programming paradigm concerned with data streams and the propagation of change. In the previous lab, we developed Inventory, Catalog, Shopping Cart and Order services with obvious interactions.

In this lab, we’ll change our shopping cart and order implementation and add a payment service as an Event-Driven/Reactive application in our cloud-native application architecture. These cloud-native applications will use AMQ Streams (based on Apache Kafka) as a messaging/streaming backbome. AMQ Streams makes it easy to run Apache Kafka on OpenShift with key features:

* Designed for horizontal scalability
* Message ordering guarantee at the partition level
* Message rewind/replay - _Long term_ storage allows the reconstruction of an application state by replaying the messages

=== Goals of this lab

The goal is to develop advanced cloud-native applications on *Red Hat Runtimes* and deploy them on *OpenShift 4* including *AMQ Streams* for distributed messaging capabilities. After this lab, you should end up with something like:

image::lab2-goal.png[goal, 700]

Scalability is one of the flagship features of Apache Kafka. It is achieved by partitioning the data and distributing them across multiple brokers. Such data sharding also has a big impact on how clients connect and use the broker. This is especially visible when Kafka is running within a platform like Kubernetes but is accessed from outside of that platform.

https://strimzi.io/[Strimzi] is an open source project that provides container images and operators for running https://developers.redhat.com/videos/youtube/CZhOJ_ysIiI/[Apache kafka].

In this lab, we will use productized and supported versions of the Strimzi and Apache Kafka projects through https://www.redhat.com/en/technologies/jboss-middleware/amq?extIdCarryOver=true&sc_cid=701f2000001OH7TAAW[Red Hat AMQ^].

=== 1. Create a Kafka Cluster and Topics

AMQ Streams is already installed using the following _Operators_ so you don’t need to install it in this lab:

* *Kafka Operator* - Responsible for deploying and managing Apache Kafka clusters within an OpenShift cluster.
* *Topic Operator* - Responsible for managing Kafka topics within a Kafka cluster running within an OpenShift cluster.
* *User Operator* - Responsible for managing Kafka users within a Kafka cluster running within an OpenShift cluster.

The basic architecture of operators in AMQ is seen below:

image::kafka-operators-arch.png[amqstreams, 700]

Let's create a **Kafka cluster**. Click on *+Add to Project* icon in the _Topology_ view. Then, type in `kafka` in the search box. Click on *Create*:

image::kafka-catalog.png[kafka, 700]

You will enter the _Form view_ that defines a *Kafka* Cluster. Keep the all values as-is then click on *Create*:

image::kafka-create-detail.png[kafka, 700]

Next, we will create _Kafka Topic_. Click _+Add to Project_ icon again, type in `topic` in the search box. Click on *Create*:

image::kafka-topic-catalog.png[kafka, 700]

You will enter YAML editor that defines a *KafkaTopic* object. Change the name to `orders` as shown then click on *Create* on the bottom:

image::kafka-topic-orders-create.png[kafka, 700]

Create another topic using the same process as above, but called `payments`:

image::kafka-topic-catalog.png[kafka, 700]

Change the name to `payments` then click on *Create* on the bottom.

image::kafka-topic-payments-create.png[kafka, 700]

*Well done!* You now have a running Kafka cluster with two Kafka Topics called `payments` and `orders`.

image::kafka-topics-created.png[kafka, 700]

=== 2. Develop and Deploy Payment Service

Our _Payment Service_ will offer online services for accepting electronic payments by a variety of payment methods including credit card or bank-based payments when orders are checked out in shopping cart. It doesn’t really do anything but will represent a payment microservice that will *process* online shopping orders as they are posted to our services.

In CodeReady Workspaces, expand *payment-service* directory.

image::codeready-workspace-payment-project.png[catalog, 700]

In this step, we will learn how our Quarkus-based payment service can use Kafka to receive order events and _react_ with payment events.

Let's add Maven Dependencies using Quarkus Kafka extensions in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="messaging-kafka" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

You should see in the output:

[source,console]
----
[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-smallrye-reactive-messaging-kafka has been installed
----

This command imports the Kafka extensions for Quarkus applications and provides all the necessary capabilities to integrate with Kafka clusters. It adds this to `pom.xml` for you:

[source,xml]
----
    ...
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
    </dependency>
    ...
----

Let’s start by adding injecting configuration using *@ConfigProperty* and a *Producer* field which will be used to send messages. We’ll also add a `log` field so we can see debug messages later on.

Add this code to the `PaymentResource.java` file (in the `src/main/java/com/redhat/cloudnative` directory) at the `// TODO: Add Messaging ConfigProperty here` marker:

[source,java,role="copypaste"]
----
    @ConfigProperty(name = "mp.messaging.outgoing.payments.bootstrap.servers")
    public String bootstrapServers;

    @ConfigProperty(name = "mp.messaging.outgoing.payments.topic")
    public String paymentsTopic;

    @ConfigProperty(name = "mp.messaging.outgoing.payments.value.serializer")
    public String paymentsTopicValueSerializer;

    @ConfigProperty(name = "mp.messaging.outgoing.payments.key.serializer")
    public String paymentsTopicKeySerializer;

    private Producer<String, String> producer;

    public static final Logger log = LoggerFactory.getLogger(PaymentResource.class);
----

Next, we need a method to handle incoming events, which in this lab will be coming directly from Kafka, but later will come through as HTTP POST events.

Add this code at the `// TODO: Add handleCloudEvent method here` marker:

[source,java,role="copypaste"]
----
    @POST
    public void handleCloudEvent(String cloudEventJson) {
        String orderId = "unknown";
        String paymentId = "" + ((int)(Math.floor(Math.random() * 100000)));

        try {
            log.info("received event: " + cloudEventJson);
            JsonObject event = new JsonObject(cloudEventJson);
            orderId = event.getString("orderId");
            String total = event.getString("total");
            JsonObject ccDetails = event.getJsonObject("creditCard");
            String name = event.getString("name");

            // fake processing time
            Thread.sleep(5000); // <1>
            if (!ccDetails.getString("number").startsWith("4")) {
                fail(orderId, paymentId, "Invalid Credit Card: " + ccDetails.getString("number"));
            } else {
                pass(orderId, paymentId, "Payment of " + total + " succeeded for " + name + " CC details: " + ccDetails.toString());
            }
        } catch (Exception ex) {
             fail(orderId, paymentId, "Unknown error: " + ex.getMessage() + " for payment: " + cloudEventJson);
        }
    }
----
<1> This will simulate a 5 second credit card processing time

Now we need to implement the `pass()` and `fail()` methods referenced above. These methods will send messages to Kafka using our _producer_ field.

Add the following code to the `// TODO: Add pass method here` marker:

[source,java,role="copypaste"]
----
    private void pass(String orderId, String paymentId, String remarks) {

        JsonObject payload = new JsonObject();
        payload.put("orderId", orderId);
        payload.put("paymentId", paymentId);
        payload.put("remarks", remarks);
        payload.put("status", "COMPLETED");
        log.info("Sending payment success: " + payload.toString());
        producer.send(new ProducerRecord<String, String>(paymentsTopic, payload.toString()));
    }
----

Add this code to the `// TODO: Add fail method here` marker:

[source,java,role="copypaste"]
----
    private void fail(String orderId, String paymentId, String remarks) {
        JsonObject payload = new JsonObject();
        payload.put("orderId", orderId);
        payload.put("paymentId", paymentId);
        payload.put("remarks", remarks);
        payload.put("status", "FAILED");
        log.info("Sending payment failure: " + payload.toString());
        producer.send(new ProducerRecord<String, String>(paymentsTopic, payload.toString()));
    }
----

Next, add a method that will receive events from Kafka. We will use the MicroProfile reactive messaging API `@Incoming` annotation to do this.

Add this code to the `// TODO: Add consumer method here` marker:

[source,java,role="copypaste"]
----
    @Incoming("orders")
    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
            throws IOException {

        log.info("Kafka message with value = {} arrived", message.getPayload());
        handleCloudEvent(message.getPayload());
        return message.ack();
    }
----

And finally, we need a method to initialize the Kafka producer (the consumer will be initialized automatically via Quarkus Kafka extension). We will use the Quarkus `StartupEvent` Lifecycle listener API, with the `@Observes` annotation to mark this method as one that should run when the app starts:

Add this code to the `// TODO: Add init method here` marker:

[source,java,role="copypaste"]
----
    public void init(@Observes StartupEvent ev) {
        Properties props = new Properties();

        props.put("bootstrap.servers", bootstrapServers);
        props.put("value.serializer", paymentsTopicValueSerializer);
        props.put("key.serializer", paymentsTopicKeySerializer);
        producer = new KafkaProducer<String, String>(props);
    }
----

This method will consume Kafka streams from the `orders` topic and call our `handleCloudEvent()` method. Later on we’ll delete this method and use Knative Events to handle the incoming stream. But for now we’ll use this method to listen to the topic.

Quarkus and its extensions are configured by an `application.properties` file. Open this file (it is in the `src/main/resources` directory).

Add these values to the file at the `# TODO: Add for messaging configuration` marker:

[source,properties,role="copypaste"]
----
# Outgoing stream
mp.messaging.outgoing.payments.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.outgoing.payments.connector=smallrye-kafka
mp.messaging.outgoing.payments.topic=payments
mp.messaging.outgoing.payments.value.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.payments.key.serializer=org.apache.kafka.common.serialization.StringSerializer

# Incoming stream (unneeded when using Knative events)
mp.messaging.incoming.orders.connector=smallrye-kafka
mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.incoming.orders.group.id=payment-order-service
mp.messaging.incoming.orders.auto.offset.reset=earliest
mp.messaging.incoming.orders.enable.auto.commit=true
mp.messaging.incoming.orders.request.timeout.ms=30000
quarkus.vertx.max-event-loop-execute-time=30000
----

Build and deploy the project using using the OpenShift extension, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----
The output should end with `BUILD SUCCESS`.

Make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/payment
----

Wait for that command to report *replication controller _payment-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/payment app.kubernetes.io/part-of=payment --overwrite && \
oc annotate dc/payment app.openshift.io/connects-to=my-cluster --overwrite && \
oc annotate dc/payment app.openshift.io/vcs-ref=ocp-4.9 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the orders. Ensure you get the blue circles!

image::payment-topology.png[order, 700]

In order to test the payment application, click on the `my-cluster-kafka-0` pod:

image::my-cluster-kafka-0.png[payment, 700]

We will watch the Kafka topic via a CLI to confirm the messages are being sent/received in Kafka. Click on the *Terminal* tab in OpenShift (not in CodeReady!) then execute the following command:

[source,sh,role="copypaste"]
----
bin/kafka-console-consumer.sh --topic payments --bootstrap-server localhost:9092
----

image::kafka-console-consumer.png[payment, 900]

Keep this tab open to act as a debugger for Kafka messages.

Now, back in *CodeReady Workspaces*, open a new Terminal.

Let’s produce a new topic message using `curl`:

First, fetch the URL of our new payment service and store it in an environment variable:

[source,sh,role="copypaste"]
----
export URL=http://$(oc get route -n {{ USER_ID}}-cloudnativeapps payment -o jsonpath={% raw %}"{.spec.host}"{% endraw %})
----

Then execute this to HTTP POST a message to our payment service with an example order:

[source,sh,role="copypaste"]
----
curl -i -H 'Content-Type: application/json' -X POST -d'{"orderId": "12321","total": "232.23", "creditCard": {"number": "4232454678667866","expiration": "04/22","nameOnCard": "Jane G Doe"}, "billingAddress": "123 Anystreet, Pueblo, CO 32213", "name": "Jane Doe"}' $URL
----

This will return in 5 seconds (our fake credit card processing time).

The payment service will recieve this _order_ and produce a _payment_ result on the Kafka _payment_ topic. You will see the
following result in `Pod Terminal`:

[source,shell]
----
{"orderId":"12321","paymentId":"25658","remarks":"Payment of 232.23 succeeded for Jane Doe CC details: {\"number\":\"4232454678667866\",\"expiration\":\"04/22\",\"nameOnCard\":\"Jane G Doe\"}","status":"COMPLETED"}
----


This shows that the `order-service` can successfully process orders in the backend and send them to the payment processor.

Before moving to the next step, stop the Kafka consumer console via kbd:[CTRL+C]:

image::kafka-console-consumer-stop.png[payment, 900]

=== 3. Adding Kafka Client to Cart Service

By now we have added several microservices to operate on our retail shopping data. Quite often, other services or functions would need the data we are working with. e.g. once a user checks out, there are other services like an _Order Service_ and our _Payment Service_ that will need this information, and would most likely want to process further. So we will integrate our Cart service
with Kafka so that it can send an order message when a shopper checks out.

Add Maven Dependencies using Quarkus *Kafka* Extensions:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="messaging-kafka" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service
----

You should see in the output:

[source,console]
----
[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-smallrye-reactive-messaging-kafka has been installed
----

This will add the Kafka extension and APIs to our Cart service app.

Like our Payment service, add this code to the `// TODO: Add annotation of orders messaging configuration here` marker inside the `CartResource` class inside the `com.redhat.cloudnative` package in the `cart-service` subdirectory of your workspace:

[source,java,role="copypaste"]
----
    @ConfigProperty(name = "mp.messaging.outgoing.orders.bootstrap.servers")
    public String bootstrapServers;

    @ConfigProperty(name = "mp.messaging.outgoing.orders.topic")
    public String ordersTopic;

    @ConfigProperty(name = "mp.messaging.outgoing.orders.value.serializer")
    public String ordersTopicValueSerializer;

    @ConfigProperty(name = "mp.messaging.outgoing.orders.key.serializer")
    public String ordersTopicKeySerializer;

    private Producer<String, String> producer;
----

Next, `un-comment` (or add if they are missing) the following `import` statements:

[source,java]
----
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.header.internals.RecordHeaders;
----

The `init()` method creates the Kafka configuration, and we have externalized this configuration and injected the variables as properties on the class. Replace the empty `init()` method with this code:

[source,java,role="copypaste"]
----
    public void init(@Observes StartupEvent ev) {
        Properties props = new Properties();

        props.put("bootstrap.servers", bootstrapServers);
        props.put("value.serializer", ordersTopicValueSerializer);
        props.put("key.serializer", ordersTopicKeySerializer);
        producer = new KafkaProducer<String, String>(props);
    }
----

The `sendOrder()` method is quite simple, it takes the Order POJO as a param and serializes that into JSON to send over the Kafka
topic. Replace the empty `sendOrder()` method with this code:

[source,java,role="copypaste"]
----
    private void sendOrder(Order order, String cartId) {
        order.setTotal(shoppingCartService.getShoppingCart(cartId).getCartTotal() + "");
        ProducerRecord<String, String> producerRecord = new ProducerRecord<>(ordersTopic, null, null, null, Json.encode(order), new RecordHeaders().add("content-type", "application/json".getBytes()));
        producer.send(producerRecord);
        log.info("Sent message: " + Json.encode(order));
    }
----

Now that we have those methods, Let's add a call to our `sendOrder()` method when we are checking out. Replace the code for `checkout()` with this code:

[source,java,role="copypaste"]
----
    @POST
    @Path("/checkout/{cartId}")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    public ShoppingCart checkout(@PathParam("cartId") String cartId, Order order) {
        sendOrder(order, cartId);
        return shoppingCartService.checkout(cartId);
    }
----

Almost there! Next let’s add the configuration to our `application.properties` file at the `# TODO: Add Kafka messaging keys and values here` marker: (in the `src/main/resources` of the `cart-service` project):

[source,none,role="copypaste"]
----
mp.messaging.outgoing.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.outgoing.orders.connector=smallrye-kafka
mp.messaging.outgoing.orders.topic=orders
mp.messaging.outgoing.orders.value.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.orders.key.serializer=org.apache.kafka.common.serialization.StringSerializer
----

Re-package the _cart service_ using the following command, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service && \
oc label dc/cart app.kubernetes.io/part-of=cart --overwrite &&  \
oc annotate dc/cart app.openshift.io/connects-to=my-cluster,datagrid-service --overwrite
----

=== 4. Adding Kafka Client to Order Service

Like the *payments* service, our *order* service will listen for orders being placed, but will not process payments - instead the order service will merely record the orders and their states for eventual display in the UI. Let’s add this capability to the order service.

Add Maven Dependencies using Quarkus *Kafka* Extensions:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="messaging-kafka" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service
----

You should see in the output:

[source,console]
----
[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-smallrye-reactive-messaging-kafka has been installed
----

This command generates a Maven project, importing the Kafka extensions for Quarkus applications and provides all the necessary capabilities to integrate with Kafka and subscribe to the _payments_ and _orders_ topic.

In the Order service, to process the incoming and outgoing messages, Create a new Java class named `KafkaOrders.java` in the `order-service/src/main/java/com/redhat/cloudnative` directory to consume messages from the Kafka _orders_ and _payments_ topic. Copy the following entire code into _KafkaOrders.java_.

[source,java,role="copypaste"]
----
package com.redhat.cloudnative;

import io.smallrye.reactive.messaging.kafka.KafkaRecord;
import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.enterprise.context.ApplicationScoped;

import java.io.IOException;
import java.util.concurrent.CompletionStage;

import javax.inject.Inject;
import io.vertx.core.json.JsonObject;

@ApplicationScoped
public class KafkaOrders {

    private static final Logger LOG = LoggerFactory.getLogger(KafkaOrders.class);

    @Incoming("orders")
    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
            throws IOException {

        LOG.info("Kafka order message with value = {} arrived", message.getPayload());

        JsonObject payload = new JsonObject(message.getPayload());
        Order newOrder = new Order();
        newOrder.orderId = payload.getString("orderId");
        newOrder.name = payload.getString("name");
        newOrder.total = payload.getString("total");
        newOrder.ccNumber = payload.getJsonObject("creditCard").getString("number");
        newOrder.ccExp = payload.getJsonObject("creditCard").getString("expiration");
        newOrder.billingAddress = payload.getString("billingAddress");
        newOrder.status = "PROCESSING";
        newOrder.persist();

        return message.ack();
    }

    @Incoming("payments")
    public CompletionStage<Void> onMessagePayments(KafkaRecord<String, String> message)
            throws IOException {

        LOG.info("Kafka payment message with value = {} arrived", message.getPayload());

        JsonObject payments = new JsonObject(message.getPayload());

        Order newOrder = Order.findByOrderId(payments.getString("orderId"));
        newOrder.status = payments.getString("status");
        newOrder.update();

        return message.ack();
    }

}
----

Almost there; Next Let's add the configuration to our `src/main/resources/application.properties` file at the `# TODO: Add for messaging configuration` marker in the _order-service_ project:

[source,none,role="copypaste"]
----
# Incoming payment topic messages
mp.messaging.incoming.payments.connector=smallrye-kafka
mp.messaging.incoming.payments.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.payments.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.payments.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.incoming.payments.group.id=order-service
mp.messaging.incoming.payments.auto.offset.reset=earliest
mp.messaging.incoming.payments.enable.auto.commit=true
mp.messaging.incoming.payments.request.timeout.ms=30000

# Enable CORS requests from browsers
quarkus.http.cors=true

# Incoming order topic messages
mp.messaging.incoming.orders.connector=smallrye-kafka
mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
mp.messaging.incoming.orders.group.id=order-service
mp.messaging.incoming.orders.auto.offset.reset=earliest
mp.messaging.incoming.orders.enable.auto.commit=true
mp.messaging.incoming.orders.request.timeout.ms=30000
----

Re-package the order service using the following command, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service && \
oc label dc/order app.kubernetes.io/part-of=order --overwrite &&  \
oc annotate dc/order app.openshift.io/connects-to=my-cluster,order-database --overwrite
----

Wait for all of the rebuilds and redeployments to complete, and you have all your blue circles!

image::coolstore-ui_event-topology.png[coolstore-ui, 700]

Let’s confirm if the all services works correctly using `Kafka` messaging via coolstore GUI test.

=== 5. End to End Functional Testing

Let’s go shopping! Access the http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^]!

Add some cool items to your cart in the following shopping scenarios:

[arabic]
. Add a _Quarkus T-shirt_ to your cart by clicking on *Add to Cart*. You will see the `Success! Added!` message under the top menu.

image::add-to-cart.png[serverless, 1000]

[arabic, start=2]
. Go to the *Cart* tab and click on the *Checkout* button . Input the credit card information. The Card Info should be 16 digits and begin with the digit `4`. For example `4123987754646678`.

[NOTE]
====
Our fake credit card processor will look for credit card numbers that begin with `4`. Any credit card number that does not start with `4` will result in a `FAILED` processing. This is a good way to check the logic of our code!
====

image::checkout.png[serverless, 1000]

[arabic, start=3]
. Input your Credit Card information to pay for the items:

image::input-cc-info.png[serverless, 1000]

[arabic, start=4]
. Confirm the _Payment Status_ of the your shopping items in the *Orders* tab. It should be `Processing`.

image::payment-processing.png[serverless, 1000]

[arabic, start=5]
. After a few moments, reload the *Orders* page to confirm that the Payment Status changed to `COMPLETED` or `FAILED`.

[NOTE]
====
If the status is still `Processing`, the order service is processing incoming Kafka messages and storing them in MongoDB. Please reload the page a few times more.
====

image::payment-completedorfailed.png[serverless, 1000]

=== Summary

In this scenario we developed an _Event-Driven/Reactive_ cloud-native application to deal with data streams from the shopping cart service to the order service and payment service using _Apache Kafka_. We also used Quarkus and its _Kafka extension_ to integrate the app with Kafka. _AMQ Streams_, a fully supported Kafka solution from Red Hat, enables you to create Apache Kafka clusters very easily via the OpenShift developer catalog.

We now have message-driven microservices for implementing reactive systems, where all the components interact using asynchronous message passing for greater reliability, scalability, and faster time to market. Most importantly, Quarkus is perfectly suited to implement event-driven microservices and reactive systems. *Congratulations!*