= Lab1 - Creating High-performing Cacheable Service
:experimental:
:imagesdir: images

In this lab, we’ll develop 5 microservices into the cloud-native application architecture. These cloud-native applications will have transactions with multiple datasources such as *PostgreSQL* and *MongoDB*. Especially, we will learn how to configure datasources easily using *Quarkus Extensions*. In the end, we will optimize *data transaction performance* of the shopping cart service thru integrating with a *Cache(Data Grid) server* to increase end users’(customers) satisfaction. And there’s more fun facts how easy it is to deploy applications on OpenShift 4 via *oc* command line tool.

=== Goals of this lab

The goal is to develop advanced cloud-native applications on *Red Hat Runtimes* enabled with _distributed cache management_ and deploy them to *OpenShift 4*. After this lab, you should end up with something like:

image::lab1-goal.png[goal, 700]

=== 1. Deploying Inventory Service

_Inventory Service_ serves inventory and availability data for retail products. Let’s go through quickly how the inventory service works and built on *Quarkus* Java runtimes. Go to _Explorer: /projects_ in _VS Code_ Web IDE and expand *inventory-service* directory.

image::codeready-workspace-inventory-project.png[inventory_service, 700]

While the code is surprisingly simple, under the hood this is using:

* *RESTEasy* to expose the REST endpoints
* *Hibernate ORM* with Panache to perform the CRUD operations on the database
* *Maven* Java project structure

_Hibernate ORM_ is the de facto JPA implementation and offers you the full breadth of an Object Relational Mapper. It makes complex mappings possible, but it does not make simple and common mappings trivial. Hibernate ORM with Panache focuses on making your entities trivial and fun to write in Quarkus.

When you open `Inventory.java` in `src/main/java/com/redhat/cloudnative/` as below, you will understand how easy to create a domain model using Quarkus extension(https://quarkus.io/guides/hibernate-orm-panache-guide[Hibernate ORM with Panache^]).

[source,java]
----
@Entity
@Cacheable
public class Inventory extends PanacheEntity {

    public String itemId;
    public String location;
    public int quantity;
    public String link;

    public Inventory() {

    }

}
----

* By extending `PanacheEntity` in your entities, you will get an ID field that is auto-generated. If you require a custom ID strategy, you can extend `PanacheEntityBase` instead and handle the ID yourself.
* By using Use public fields, there is no need for functionless getters and setters (those that simply get or set the field). You simply refer to fields like `Inventory.location` without the need to write a `Inventory.getLocation()`` implementation. Panache will auto-generate any getters and setters you do not write, or you can develop your own getters/setters that do more than get/set,
which will be called when the field is accessed directly.

The `PanacheEntity` superclass comes with lots of super useful static methods and you can add your own in your derived entity class, and much like traditional object-oriented programming it’s natural and recommended to place custom queries as close to the entity as possible, ideally within the entity definition itself. Users can just start using your Inventory entity by typing
`Inventory.`, and getting completion for all the operations in a single place.

When an entity is annotated with `@Cacheable`, all its field values are cached except for collections and relations to other entities. This means the entity can be loaded without querying the database, but be careful as it implies the loaded entity might not reflect recent changes in the database.

Next, let’s find out how the inventory service exposes _RESTful APIs_ in Quarkus. Open `InventoryResource.java` in `src/main/java/com/redhat/cloudnative/` and you will see the following code snippet.

The REST services defines two endpoints:

* `/api/inventory` that is accessible via `HTTP GET` which will return all known product Inventory entities as JSON
* `/api/inventory/<itemId>` that is accessible via `HTTP GET` at for example `/inventory/329199` with the last path parameter
being the location which we want to check its inventory status.

image::inventoryResource.png[inventory_service, 700]

*In Development*, we will configure to use local _in-memory H2 database_ for local testing, as defined in `src/main/resources/application.properties`:

[source,none]
----
%dev.quarkus.datasource.db-kind=h2
%dev.quarkus.datasource.username=inventory
%dev.quarkus.datasource.password=mysecretpassword
%dev.quarkus.datasource.jdbc.max-size=8
%dev.quarkus.datasource.jdbc.min-size=2
%dev.quarkus.hibernate-orm.database.generation=drop-and-create
%dev.quarkus.hibernate-orm.log.sql=false
----

Let’s run the inventory application locally using `maven plugin command` via VS Code Terminal:

[source,sh,role="copypaste"]
----
mvn quarkus:dev -Dquarkus.http.host=0.0.0.0 -f $PROJECT_SOURCE/inventory-service
----

You should see a bunch of log output that ends with:

[source,console]
----
__  ____  __  _____   ___  __ ____  ______ 
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
INFO  [io.agr.pool] (Quarkus Main Thread) Datasource '<default>': Initial size smaller than min. Connections will be created when necessary

INFO  [io.quarkus] (Quarkus Main Thread) inventory 1.0-SNAPSHOT on JVM (powered by Quarkus 2.13.7.Final-redhat-00003) started in 5.859s. Listening on: http://0.0.0.0:8080
INFO  [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated.
INFO  [io.quarkus] (Quarkus Main Thread) Installed features: [agroal, cdi, hibernate-orm, hibernate-orm-panache, jdbc-h2, kubernetes, narayana-jta, resteasy, resteasy-jackson, smallrye-context-propagation, smallrye-health, vertx]

--
Tests paused
Press [r] to resume testing, [o] Toggle test output, [:] for the terminal, [h] for more options>
----

You probably noticed that there was magic and we didn't specify the JDBC URL, this is because of https://quarkus.io/guides/databases-dev-services[Quarkus Dev Services^] and it works for all DBs, but here we can't have containers so we've done in-memory H2.

When you take a look at the Quarkus Dev Mode terminal, you should see the following logs:

[source,sh]
----
Dev Services for the default datasource (h2) started.
----

Then, press `c` to find more information about the H2 Dev Services.

[source,sh]
----
== Dev Services

  Injected Config:  quarkus.datasource.password=sa, quarkus.datasource.db-kind=h2, quarkus.datasource.jdbc.url=jdbc:h2:tcp://localhost:42165/mem:quarkus;DB_CLOSE_DELAY=-1, quarkus.datasource.username=sa
----

You can also see *Tests paused* by default when a Quarkus application gets started.

VS Code will also detect that the Quarkus app opens port `5005` (for debugging) and `8080` (for web requests). *Close the popup not to add a port 5005*, but when prompted, *Open In New Tab* to open a port `8080`, which opens a new tab in your web browser:

image::open-port.png[port, 700]

[NOTE]
====
In case you see the popup message below, select `Open`.

image::open-external.png[port, 700]
====

You should see the *Coolstore Inveotry* page:

image::coolstore-inventory.png[port, 900]

Open a *new* terminal by selecting `+` icon:

image::cmd-terminal.png[livecoding, 900]

and invoke the RESTful endpoint using the following CURL commands.

[source,sh,role="copypaste"]
----
curl http://localhost:8080/api/inventory | jq
----

The output looks like:

[source,json]
----
  ...
  {
    "id": 7,
    "itemId": "444435",
    "location": "Paris",
    "quantity": 600,
    "link": "http://maps.google.com/?q=Paris"
  },
  {
    "id": 8,
    "itemId": "444437",
    "location": "Tokyo",
    "quantity": 230,
    "link": "http://maps.google.com/?q=Tokyo"
  }
]
----

===== Controlling Continuous Testing

There are various hotkeys you can use to control continuous testing. Pressing `h` will display the following list of commands in the terminal:

[source, none]
----
== Continuous Testing

[r] - Resume testing
[o] - Toggle test output (disabled)

== Dev Services

[c] - Show dev services containers
[g] - Follow dev services logs to the console (disabled)

== Exceptions

[x] - Opens last exception in IDE (None)

== HTTP

[w] - Open the application in a browser
[d] - Open the Dev UI in a browser

== System

[s] - Force restart
[i] - Toggle instrumentation based reload (disabled)
[l] - Toggle live reload (enabled)
[j] - Toggle log levels (INFO)
[h] - Shows this help
[:] - Enters terminal mode
[q] - Quits the application

--
Tests paused
Press [r] to resume testing, [o] Toggle test output, [:] for the terminal, [h] for more options>
----

Quarkus also provides a new experimental Dev UI, which is available in dev mode (when you start quarkus with mvn quarkus:dev) at `/q/dev` by default. It allows you to quickly visualize all the extensions currently loaded, see their status and go directly to their documentation.

Click on `quarkus-devui` endpoint:

image::devui-endpoint.png[devui-endpoint, 700]

A new web browser will open automatically then it will show you something like this:

image::quarkus-devui.png[devui-endpoint, 800]

Be sure to terminate the running Quarkus development via kbd:[CTRL+C].

*In production*, the inventory service will connect to _PostgeSQL_ on OpenShift cluster.

We will use _Quarkus extension_ to add *PostgreSQL JDBC Driver*. Go back to VS Code Terminal and run the following
maven plugin:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="jdbc-postgresql" -f $PROJECT_SOURCE/inventory-service
----

You should see in the output:

[source,console]
----
[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-jdbc-postgresql has been installed
----

First, open a new brower with the {{ CONSOLE_URL }}[OpenShift web console^]:

image::openshift_login.png[openshift_login, 700]

Login using:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

You will see a list of projects to which you have access:

image::openshift_landing.png[openshift_landing, 700]

[NOTE]
====
The project displayed in the landing page depends on which labs you will run today. If you will develop
`Service Mesh and Identity` then you will see pre-created projects as the above screenshot.
====

Open the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^].

Our production inventory microservice will use an external database (PostgreSQL) to house inventory data. First, deploy a new
instance of PostgreSQL. *Right-Click* on the _Topology_ view. Then, click on _Database_ in the *Add to Project* popup menu in *{{ USER_ID }}-cloudnativeapps* project:

image::db.png[db, 700]

Type in `postgres` in the search box, and click on the *PostgreSQL (ephemeral)*:

image::db-postgres.png[db, 700]

Click on *Instantiate Template* and fill in the following fields, leaving the others as their default values:

* *Namespace*: _choose `{{ USER_ID }}-cloudnativeapps` for the first Namespace. Leave the second one as `openshift`_
* *Database Service Name*: `inventory-database`
* *PostgreSQL Connection Username*: `inventory`
* *PostgreSQL Connection Password*: `mysecretpassword`
* *PostgreSQL Database Name*: `inventory`

image::db-postgres-inventory-values.png[db, 700]

This will deploy the database to our new project. Click on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] to see it:

image::inventory-database-deployment.png[inventory_db_deployments, 700]

Although your VS Code is running on the Kubernetes cluster, it's running with a default restricted _Service Account_ that prevents you from creating most resource types. So we'll log in with your workshop user. Execute the following command in the VS Code terminal:

[source,sh,role="copypaste"]
----
oc login -u {{ USER_ID }} -p {{ OPENSHIFT_USER_PASSWORD }} https://openshift.default.svc:443
----

You should see something like this (the project names may be different):

[source,none]
----
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    {{ USER_ID }}-bookinfo
    {{ USER_ID }}-catalog
    {{ USER_ID }}-cloudnative-pipeline
    {{ USER_ID }}-cloudnativeapps
    {{ USER_ID }}-coolstore-dev
  * {{ USER_ID }}-devspaces
    {{ USER_ID }}-inventory
    {{ USER_ID }}-istio-system

Using project "{{ USER_ID }}-devspaces".
----

Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

[source,sh,role="copypaste"]
----
oc project {{ USER_ID }}-cloudnativeapps && \
mvn clean compile package -DskipTests -f $PROJECT_SOURCE/inventory-service
----

The output should end with `BUILD SUCCESS`.

Finally, make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/inventory
----

Wait for that command to report *replication controller _inventory-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/inventory app.kubernetes.io/part-of=inventory --overwrite && \
oc label dc/inventory-database app.kubernetes.io/part-of=inventory app.openshift.io/runtime=postgresql --overwrite && \
oc annotate dc/inventory app.openshift.io/connects-to=inventory-database --overwrite && \
oc annotate dc/inventory app.openshift.io/vcs-ref=ocp-4.13 --overwrite
----

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^], make sure it's done deploying (dark blue circle):

image::inventory_topology.png[inventory, 700]

Click on the small arrow icon as shown above and you will see inventories:

image::inventory_topology_openurl.png[inventory, 700]

So now `Inventory` service is deployed to OpenShift. You can also see it in the Project Status in the OpenShift Console with its single replica running in 1 pod, along with the Postgres database pod.

=== 2. Deploying Catalog Service

_Catalog Service_ serves products and prices for retail products. Let’s go through quickly how the catalog service works and built on *Spring Boot* Java runtimes. Go to _Explorer: /projects_ in _VS Code_ Web IDE and expand *catalog-service* directory.

image::codeready-workspace-catalog-project.png[catalog, 700]

First of all, we won’t implement the catalog application to retrieve data because all functions are already built when we imported this project from Git server. There’re a few interesting things what we need to take a look at from this Spring Boot application before we will deploy it to OpenShift cluster.

Notice that we are using the Dekorate Maven dependency to deploy your Spring Boot applications to OpenShift. The Fabric8 Maven plugin is no longer supported. For information about deploying your application to OpenShift. Dekorate is a collection of compile-time annotation processors and application resource generators that are provided with Red Hat build of Spring Boot. It works by parsing annotations in your code when you build your application, and extracting configuration properties. Dekorate then uses the extracted values of properties to generate application configuration resources that you can use to deploy your application to a Kubernetes or OpenShift cluster. Find more information here, https://access.redhat.com/documentation/en-us/red_hat_support_for_spring_boot/2.7/html-single/dekorate_guide_for_spring_boot_developers/[Dekorate Guide for Spring Boot Developers^].

[source,xml]
----
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>io.dekorate</groupId>
                <artifactId>dekorate-spring-bom</artifactId>
                <version>${dekorate.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>${spring-boot.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>
----

We use this bill of material to make sure that we are using a version of Apache Tomcat (at least in this example) that Red Hat supports.

Also, catalog service calls the inventory service that we deployed earlier using REST to retrieve the inventory status and include that in the response. Open `CatalogService.java` in `src/main/java/com/redhat/cloudnative/service` directory via Project Explorer and how `read()` and `readAll()` method work.

[source,java]
----
   public Product read(String id) {
        Product product = repository.findById(id);
        //TODO: Update the quantity for the product by calling the Inventory service
        JSONArray jsonArray = new JSONArray(inventoryClient.getInventoryStatus(product.getItemId()));
        List<String> quantity = IntStream.range(0, jsonArray.length())
            .mapToObj(index -> ((JSONObject)jsonArray.get(index))
            .optString("quantity")).collect(Collectors.toList());
        product.setQuantity(Integer.parseInt(quantity.get(0)));
        return product;
    }

    public List<Product> readAll() {
        List<Product> productList = repository.readAll();
        //TODO: Update the quantity for the products by calling the Inventory service
        for ( Product p : productList ) {
            JSONArray jsonArray = new JSONArray(inventoryClient.getInventoryStatus(p.getItemId()));
            List<String> quantity = IntStream.range(0, jsonArray.length())
                .mapToObj(index -> ((JSONObject)jsonArray.get(index))
                .optString("quantity")).collect(Collectors.toList());
            p.setQuantity(Integer.parseInt(quantity.get(0)));
        }
        return productList; 
    }
----

Our `production` catalog microservice will use an external database (PostgreSQL) to house inventory data. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^].

instance of PostgreSQL. *Right-Click* on the _Topology_ view. Then, click on _Database_ in the *Add to Project* popup menu:

image::db.png[db, 700]

Type in `postgres` in the search box, and click on the *PostgreSQL (ephemeral)*:

image::db-postgres.png[db, 700]

Click on *Instantiate Template* and fill in the following fields, leaving the others as their default values:

* *Namespace*: _choose `{{ USER_ID }}-cloudnativeapps` for the first Namespace. Leave the second one as `openshift`_
* *Database Service Name*: `catalog-database`
* *PostgreSQL Connection Username*: `catalog`
* *PostgreSQL Connection Password*: `mysecretpassword`
* *PostgreSQL Database Name*: `catalog`

image::db-catalog-postgres-fields.png[db, 700]

This will deploy the database to our catalog project. Click on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] to see it.

Build and deploy the project using the following command, which will use the maven plugin to deploy via VS Code Terminal:

[source,sh,role="copypaste"]
----
mvn clean install -Ddekorate.deploy=true -DskipTests -f $PROJECT_SOURCE/catalog-service
----

You should see a *route.route.openshift.io/catalog created* at the end of the deployment output.

Once the application is deployed, we will also give it some labels to make it look nice. Run this command:

[source,sh,role="copypaste"]
----
oc label dc/catalog app.kubernetes.io/part-of=catalog app.openshift.io/runtime=spring-boot --overwrite && \
oc label dc/catalog-database app.kubernetes.io/part-of=catalog app.openshift.io/runtime=postgresql --overwrite && \
oc annotate dc/catalog app.openshift.io/connects-to=inventory,catalog-database --overwrite && \
oc annotate dc/catalog app.openshift.io/vcs-uri=https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs.git --overwrite && \
oc annotate dc/catalog app.openshift.io/vcs-ref=ocp-4.13 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the catalog, and ensure you get the blue circles! You will also see the icon of the *Red Hat Support for Spring Boot*.

image::inventory-catalog-topology.png[catalog, 700]

And then access the http://catalog-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Catalog Web frontend^] and ensure you get the expected inventory quantity (and not `-1`), you may need to reload the page if the app isn't initialized yet:

image::catalog.png[catalog, 700]

So now `Catalog` service is deployed to OpenShift. You can also see it in the Project Status in the OpenShift Console with running 4 pods such as catalog, catalog-database, inventory, and inventory-database.

=== 3. Developing and Deploying Shopping Cart Service

By now, you have deployed some of the essential elements for the Coolstore application. However, an online shop without a cart means no checkout experience. In this section, we are going to implement the Shopping Cart; in our Microservice world, we are going to call it the *cart service* and our java artifact/repo is called the *cart-service*.

The Cart service is *Reactive RESTful* and built with Quarkus using Red Hat’s Distributed _Data Grid_ technology. It stores all shopping cart data and assigns a unique id to each. It uses the Quarkus _Infinispan client_ to do this (_Infinispan_ is the name of the upstream project that Red Hat Data Grid is based on). The Shopping cart makes a call via the Quarkus reactive REST client to fetch all items in the Catalog. In the end, Shopping cart also pushes messages to a _Kafka_ for each order, when the user is checking out. For that, we use the Quarkus Kafka client.

What is a _Shopping Cart_ in our context? A Shopping cart has a list of Shopping Items. Each item has a _quantity_, and other fields like discounts and promotional details. We will see these in more detail when we look at our model.

For this lab, we are using VS Code. Make sure you have the following project open in your workspace. Let’s go through quickly how the cart service works and is built on _Quarkus_ Java runtimes. Go to _Explorer_ in VS Code and expand the *cart-service* directory.

image::codeready-workspace-cart-project.png[cart, 700]

We are going to use the Red Hat Distributed _Data Grid_ for caching all the users' carts.

Let's create a simple version of the *cache service* in our cluster. Open the Terminal in your VS Code and run the following command:

[source,sh,role="copypaste"]
----
oc new-app --as-deployment-config quay.io/openshiftlabs/ccn-infinispan:12.0.0.Final-1 --name=datagrid-service -e USER=user -e PASS=pass
----

This will create a single instance of the Data Grid server to store our shopping carts.

Click on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] to see it.

Now that our cache service(a.k.a datagrid-service) is deployed. We want to ensure that everything in our cart is persisted in this blazing fast cache. It will help us when we have a few million users per second on a black Friday.

Following is what we need to do:

* Model our data
* Choose how we store the data
* Create a marshaller for our data
* Inject our cache connection into the service

We have made this choice easier for you using an annotation based serialization. Let’s take a look at our `Product.java` class file in `cart-service/src/main/java/com/redhat/cloudnative/model`:

[source,java]
----
...
    @ProtoFactory
    public Product(String itemId, String name, String desc, double price) {
        super();
        this.itemId = itemId;
        this.name = name;
        this.desc = desc;
        this.price = price;
    }

    @ProtoField(number = 1)
    public String getItemId() {
        return itemId;
    }
    public void setItemId(String itemId) {
        this.itemId = itemId;
    }
...
----

This can be done automatically by adding protostream annotations(*@ProtoFactory*, *@ProtoField*) to the _Product_ class. In addition a single _Initializer_ annotated interface is required which controls how the supporting classes are generated.

Then all that is required is a very simple *SerializationContextInitializer* interface with an annotation on it to specify configuration settings.

Create a new Java class called `CartContextInitializer.java` in `com.redhat.cloudnative.model` and copy the below code into the file:

[source,java,role="copypaste"]
----
package com.redhat.cloudnative.model;

import org.infinispan.protostream.SerializationContextInitializer;
import org.infinispan.protostream.annotations.AutoProtoSchemaBuilder;

@AutoProtoSchemaBuilder (includeClasses = {ShoppingCart.class, ShoppingCartItem.class, Promotion.class, Product.class }, schemaPackageName = "coolstore")
interface CartContextInitializer extends SerializationContextInitializer {

}
----

*Perfect!* Now we have all the building blocks ready to use the cache. Let's start using our cache.

Next we need to make sure we will inject our cache in our service. Open `com.redhat.cloudnative.service.ShoppingCartServiceImpl` and add this at the `// TODO Inject RemoteCache` marker:

[source,java,role="copypaste"]
----
    @Inject
    @Remote(CacheService.CART_CACHE)
    RemoteCache<String, ShoppingCart> carts;
----

The cart is quite simple; All the information from the browser i.e., via our *Angular App* is via _JSON_ at the _/api/cart_ endpoint:

* `GET {cartId}` gets the items in the cart, or creates a new unique ID if one is not present.
* `POST {cartId}/{itemId}/{quantity}` will add items to the cart.
* `DELETE {cartId}/{itemId}/{quantity}` will remove items from the cart.
* `POST checkout/{cartId}` will remove the items and invoke the checkout procedure.

Let’s take a look at how we do this with Quarkus. In our *cart-service* project and in our main package i.e., `com.redhat.cloudnative` is the `CartResource`.

At the `// TODO ADD getCart method` marker, add this method:

[source,java,role="copypaste"]
----
    public ShoppingCart getCart(String cartId) {
        return shoppingCartService.getShoppingCart(cartId);
    }
----

The code above is using the `ShoppingCartService`, which is injected into the `CartResource` via the Dependency Injection. The `ShoppingCartService` take a `cartId` as a parameter and returns the associated ShoppingCart. So that’s perfect, however, for our Endpoint i.e., CartResource to respond, we need to define a couple of things:

* The type of HTTPRequest
* The type of data it can receive
* The path it resolves too

Add the following code on top of the `getCart` method

[source,java,role="copypaste"]
----
    @GET
    @Path("{cartId}")
----

We have now successfully stated that the method adheres to a GET request and accepts data in *plain text*. 

Take this opportunity to look at some of the other methods. You will find `@POST` and `@DELETE` and also the paths they adhere to. This is how we can construct a simple endpoint for our application.

[NOTE]
====
There are other *// TODO* markers and commented-out code we will use later. Leave them alone for now.
====

Quarkus also offers the ability to automatically generate OpenShift resources based on sane default and user supplied configuration. The OpenShift extension is actually a wrapper extension that brings together the https://quarkus.io/guides/deploying-to-kubernetes[kubernetes^] and https://quarkus.io/guides/container-image#s2i[container-image-s2i^] extensions with defaults so that it’s easier for the user to get started with Quarkus on OpenShift.

Add _openshift_ extension via VS Code Terminal:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="openshift" -f $PROJECT_SOURCE/cart-service
----

you will see:

[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-openshift has been installed

Quarkus supports the notion of _configuration profiles_. These allows you to have multiple configurations in the same file and
select between then via a _profile name_.

Let’s `add` the following variables at the `# TODO: Add for OpenShift extension` marker in _src/main/resources/application.properties_:

[source,shell,role="copypaste"]
----
quarkus.kubernetes-client.trust-certs=true<1>
quarkus.kubernetes.deploy=true<2>
quarkus.kubernetes.deployment-target=openshift<3>
quarkus.openshift.build-strategy=docker<4>
quarkus.openshift.expose=true<5>
----

<1> We are using self-signed certs in this simple example, so this simply says to the extension to trust them.
<2> Instructs the extension to deploy to OpenShift after the container image is built
<3> Instructs the extension to generate and create the OpenShift resources (like `DeploymentConfig` and `Service`) after building the container
<4> Set the Docker build strategy
<5> Instructs the extension to generate an OpenShift `Route`

*Docker build* strategy builds the artifacts (JAR files or a native executable) outside the OpenShift cluster, either locally or in a CI environment, and then provides them to the OpenShift build system together with a Dockerfile. The container is built inside the OpenShift cluster and provided as an image stream.

Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $PROJECT_SOURCE/cart-service
----

The output should end with `BUILD SUCCESS`.

Finally, make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/cart
----

Wait for that command to report *replication controller _cart-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/cart app.kubernetes.io/part-of=cart app.openshift.io/runtime=quarkus --overwrite && \
oc label dc/datagrid-service app.kubernetes.io/part-of=cart app.openshift.io/runtime=datagrid --overwrite && \
oc annotate dc/cart app.openshift.io/connects-to=catalog,datagrid-service --overwrite && \
oc annotate dc/cart app.openshift.io/vcs-ref=ocp-4.13 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the catalog, and ensure you get the blue circles!

image::cart-topology.png[catalog, 700]

And then access the http://cart-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN }}/q/swagger-ui[Cart Swagger UI^]:

image::cart-swagger-ui.png[cart, 700]

You can try to invoke the methods and see the output from the service. Hence an excellent way to test quickly as well.

=== 4. Developing and Deploying Order Service

The Order Service manages all orders when customers checkout items in the shopping cart. Let’s go through quickly how the order
service get reactive REST services to use the *MongoDB* database with *Panache* on *Quarkus* Java runtime. Go to _Explorer: /projects_ in _VS Code_ Web IDE and expand *order-service* directory.

image::codeready-workspace-order-project.png[order, 700]

The application built in _Quarkus_ is quite simple: the user can add elements in a list using _reactive RESTful APIs_ and the list is updated. All the information between the client and the server are formatted as *JSON*. The elements are stored in _MongoDB_.

Developers might have hard time to use raw APIs that query entities int the MongoDB https://mongodb.github.io/mongo-java-driver/4.2/bson/documents/#document[Document^]. 

MongoDB with Panache provides active record style entities (and repositories) for developers to have in https://quarkus.io/guides/hibernate-orm-panache[Hibernate ORM^] with Panache and focuses on making your entities trivial and fun to write in Quarkus.

Execute the following command for adding Maven Dependencies using Quarkus Extensions via VS Code Terminal:

[source,sh,role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="mongodb-panache,resteasy-reactive-jackson" -f $PROJECT_SOURCE/order-service
----

You should see in the output:

[source,console]
----
[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-mongodb-panache has been installed
[INFO] [SUCCESS] ✅  Extension io.quarkus:quarkus-resteasy-reactive-jackson has been installed
----

This command generates a Maven structure importing the RESTEasy Reactive/Jackson and MongoDB Panache extensions. After this, the quarkus-mongodb-panache extension has been added to your *pom.xml*.

image::order-pom-dependency.png[order, 700]

Note that you can use a https://quarkus.io/guides/mongodb[MongoDB Client^] extension to implement entities from MongoDB documents with a Bson Codec.

Before we create the order service using JSON REST service, let's have a look at the `Order` bean in `src/main/java/com/redhat/cloudnative/` as follows:

image::order_bean.png[order, 700]

To define a Panache entity, simply extend PanacheMongoEntity and add your columns as public fields. You can add the @MongoEntity annotation to your entity if you need to customize the name of the collection, the database, or the client.

Now, edit the `com.redhat.cloudnative.OrderResource` class as follows in each marker:

`// TODO: Add JAX-RS annotations here` marker:

[source,java,role="copypaste"]
----
@Path("/api/orders")
----

`// TODO: Add list(), add(), updateStatus() methods here` marker:

[source,java,role="copypaste"]
----
    @GET
    public List<Order> list() {
        return Order.listAll();
    }

    @POST
    public List<Order> add(Order order) {
        order.persist();
        return list();
    }

    @GET
    @Path("/{orderId}/{status}")
    public Order updateStatus(String orderId, String status) {
        Order newOrder = Order.findByOrderId(orderId);
        newOrder.status = status;
        newOrder.update();
        return newOrder;

    }
----

The implementation is pretty straightforward and you just need to define your endpoints, for example, add the io.quarkus:quarkus-resteasy-reactive-jackson dependency for JAX-RS and JSON support.

The main property to configure is the URL to access to *MongoDB*, almost all configuration can be included in the connection URI so we advise you to do so, you can find more information in the https://docs.mongodb.com/manual/reference/connection-string/[MongoDB documentation^]

Open `application.properties` in `src/main/resources/` and add the following configuration at the `# TODO: Add for MongoDB configuration` marker:

[source,sh,role="copypaste"]
----
quarkus.mongodb.connection-string=mongodb://order-database:27017
quarkus.mongodb.database=order
----

Run the following `oc` command to deploy a `MongoDB` to OpenShift via VS Code Terminal:

[source,sh,role="copypaste"]
----
oc new-app -n {{ USER_ID }}-cloudnativeapps  --as-deployment-config --docker-image quay.io/openshiftlabs/ccn-mongo:4.0 --name=order-database
----

Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $PROJECT_SOURCE/order-service
----

The output should end with `BUILD SUCCESS`.

Finally, make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/order
----

Wait for that command to report *replication controller _order-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/order app.kubernetes.io/part-of=order --overwrite && \
oc label dc/order-database app.kubernetes.io/part-of=order app.openshift.io/runtime=mongodb --overwrite && \
oc annotate dc/order app.openshift.io/connects-to=order-database --overwrite && \
oc annotate dc/order app.openshift.io/vcs-ref=ocp-4.13 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the orders. Ensure you get the blue circles!

image::order-topology.png[order, 700]

And then access the http://order-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}/api/orders[Orders^]. You will see empty result because you didn’t add any shopping items yet:

[source,sh]
----
[]
----

You can also see this with `curl` with this command in a Terminal:

[source,sh,role="copypaste"]
----
curl -s http://order-{{USER_ID}}-cloudnativeapps.{{ROUTE_SUBDOMAIN}}/api/orders | jq
----

Which will also return an empty array `[]`.

=== 5. Deploying WEB-UI Service

Our Web UI serves a frontend based on https://angularjs.org/[AngularJS^] and http://patternfly.org/[PatternFly^] running in a https://access.redhat.com/documentation/en/openshift-container-platform/3.3/paged/using-images/chapter-2-source-to-image-s2i[Node.js] container. https://www.redhat.com/en/products/runtimes[Red Hat Runtimes^] includes *Node.js* support along with other runtimes used for cloud native development.

Let’s go through quickly how the frontend service works and is built on Node.js runtimes. Go to _Explorer: /projects_ in VS Code and expand the `coolstore-ui` directory.

image::codeready-workspace-coolstore-ui.png[coolstore-ui, 700]

You will see javascript for specific cloud-native services such as the cart, catatlog, and order service as above.

Now, we will deploy a presentation layer to OpenShift cluster using https://www.npmjs.com/package/nodeshift[Nodeshift] command line tool, a programmable API that you can use to deploy Node.js projects to OpenShift.

Install Nodeshift via the VS Code Terminal:

[source,sh,role="copypaste"]
----
cd $PROJECT_SOURCE/coolstore-ui && npm install --save-dev nodeshift
----

[NOTE]
====
You may see warnings from _npm_ about known vulnerabilities. The _npm_ ecosystem is huge and our app pulls in many dependencies which are constantly finding new issues. In a real production system you should pay attention to these, but for this workshop you can ignore them (your friendly workshop developers fix them as time permits when we're not supporting our customers!).
====

Next, deploy the _coolstore-ui_ service using `Nodeshift` in a VS Code Terminal. It will take a minute to complete the deployment:

[source,sh,role="copypaste"]
----
npm run nodeshift && oc expose svc/coolstore-ui && \
oc label dc/coolstore-ui app.kubernetes.io/part-of=coolstore --overwrite && \
oc annotate dc/coolstore-ui app.openshift.io/connects-to=order-cart,catalog,inventory,order --overwrite && \
oc annotate dc/coolstore-ui app.openshift.io/vcs-uri=https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs.git --overwrite && \
oc annotate dc/coolstore-ui app.openshift.io/vcs-ref=ocp-4.13 --overwrite
----

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^], make sure it's done deploying (dark blue circle):

image::coolstore-ui_topology.png[coolstore-ui, 700]

And then access the http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^] and ensure you get the expected products and inventories:

image::web-ui-landing.png[coolstore-ui, 700]

This confirms that the frontend is properly hooked up to the backend, which is properly hooked up to our Data Grid deployment.

=== Summary

In this scenario we developed and deployed 5 microservices, each with a REST API and each of which communicate with the other microservices. We also used a variety of application runtimes such as Quarkus, Spring Boot, and Node.js to compile, package, and containerize applications -- this is an important capability of advanced cloud-native architectures.

To deploy cloud-native applications with multiple datasources on an OpenShift cluster, Quarkus provides an easy way to connect multiple datasources and obtain a reference to those datasources such as PostgreSQL and MongoDB in code.

In the end, we optimized the _transaction performance_ of the shopping cart service by integrating it with *Red Hat Data Grid* to increase end users’ (customers) satisfaction. This may not be obvious with only one user (you), but at scale these components can ensure relability and business performance. *Congratulations!*
