= Lab1 - Creating High-performing Cacheable Service
:experimental:

In this lab, we’ll develop 5 microservices into the cloud-native appliation architecture. These cloud-native applications will have transactions with multiple datasources such as *PostgreSQL* and *MongoDB*. Especially, we will learn how to configure datasources easily using *Quarkus Extensions*. In the end, we will optimize *data transaction performance* of the shopping cart service thru integrating with a *Cache(Data Grid) server* to increase end users’(customers) satification. And there’s more fun facts how easy it is to deploy applications on OpenShift 4 via *oc* command line tool.

=== Goals of this lab

The goal is to develop advanced cloud-native applications on *Red Hat Runtimes* and deploy them on *OpenShift 4* including _single sign-on access management_ and _distributed cache manageemnt_. After this lab, you should end up with something like:

image::lab1-goal.png[goal, 700]

=== 1. Deploying Inventory Service

_Inventory Service_ serves inventory and availability data for retail products. Lets’s go through quickly how the inventory service works and built on *Quarkus* Java runtimes. Go to _Explorer: /projects_ in _CodeReady Workspaces_ Web IDE and expand *inventory-service* directory.

image::codeready-workspace-inventory-project.png[inventory_service, 700]

While the code is surprisingly simple, under the hood this is using:

* *RESTEasy* to expose the REST endpoints
* *Hibernate ORM* with Panache to perform the CRUD operations on the database
* *Maven* Java project structure

_Hibernate ORM_ is the de facto JPA implementation and offers you the full breadth of an Object Relational Mapper. It makes complex mappings possible, but it does not make simple and common mappings trivial. Hibernate ORM with Panache focuses on making your entities trivial and fun to write in Quarkus.

When you open `Inventory.java` in `src/main/java/com/redhat/cloudnative/` as below, you will understand how easy to create a domain model using Quarkus extension(https://quarkus.io/guides/hibernate-orm-panache-guide[Hibernate ORM with Panache^]).

[source,java]
----
@Entity
@Cacheable
public class Inventory extends PanacheEntity {

    public String itemId;
    public String location;
    public int quantity;
    public String link;

    public Inventory() {

    }

}
----

* By extending `PanacheEntity` in your entities, you will get an ID field that is auto-generated. If you require a custom ID strategy, you can extend `PanacheEntityBase` instead and handle the ID yourself.
* By using Use public fields, there is no need for functionless getters and setters (those that simply get or set the field). You simply refer to fields like `Inventory.location` without the need to write a `Inventory.getLocation()`` implementation. Panache will auto-generate any getters and setters you do not write, or you can develop your own getters/setters that do more than get/set,
which will be called when the field is accessed directly.

The `PanacheEntity` superclass comes with lots of super useful static methods and you can add your own in your derived entity class, and much like traditional object-oriented programming it’s natural and recommended to place custom queries as close to the entity as possible, ideally within the entity definition itself. Users can just start using your Inventory entity by typing
`Inventory.`, and getting completion for all the operations in a single place.

When an entity is annotated with `@Cacheable`, all its field values are cached except for collections and relations to other entities. This means the entity can be loaded without querying the database, but be careful as it implies the loaded entity might not reflect recent changes in the database.

Next, let’s find out how the inventory service exposes _RESTful APIs_ in Quarkus. Open `InventoryResource.java` in `src/main/java/com/redhat/cloudnative/` and you will see the following code snippet.

The REST services defines two endpoints:

* `/api/inventory` that is accessible via `HTTP GET` which will return all known product Inventory entities as JSON
* `/api/inventory/<itemId>` that is accessible via `HTTP GET` at for example `/inventory/329199` with the last path parameter
being the location which we want to check its inventory status.

image::inventoryResource.png[inventory_service, 700]

*In Development*, we will configure to use local _in-memory H2 database_ for local testing, as defined in `src/main/resources/application.properties`:

[source,none]
----
%dev.quarkus.datasource.url=jdbc:h2:mem:inventory
%dev.quarkus.datasource.driver=org.h2.Driver
%dev.quarkus.datasource.username=inventory
%dev.quarkus.datasource.password=mysecretpassword
%dev.quarkus.datasource.max-size=8
%dev.quarkus.datasource.min-size=2
%dev.quarkus.hibernate-orm.database.generation=drop-and-create
%dev.quarkus.hibernate-orm.log.sql=false
----

Let’s run the inventory application locally using `maven plugin command` via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn quarkus:dev -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/inventory-service
----

You should see a bunch of log output that ends with:

[source,console]
----
2020-03-19 00:55:12,598 INFO  [io.agr.pool] (main) Datasource '<default>': Initial size smaller than min. Connections will be created when necessary
2020-03-19 00:55:12,887 INFO  [io.quarkus] (main) inventory 1.0-SNAPSHOT (running on Quarkus xx.xx.xx) started in 3.166s. Listening on: http://0.0.0.0:8080
2020-03-19 00:55:12,890 INFO  [io.quarkus] (main) Profile dev activated. Live Coding activated.
2020-03-19 00:55:12,890 INFO  [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, hibernate-orm-panache, jdbc-h2, narayana-jta, resteasy, resteasy-jsonb, smallrye-health]
----

CodeReady will also detect that the Quarkus app opens port `5005` (for debugging) and `8080` (for web requests). Do not open port 5005, but when prompted, open the port `8080`, which opens a small web browser in CodeReady:

You should see a bunch of log output and Theia popup shows the endpoint for your local application. Click on `Open Link` then you
will see *Coolstore Inventory* page on you left side. `Close` the popup window.

image::open-port.png[Inventory RESTful Service, 700]

You should see the inventory web frontend directly in CodeReady (you may need to click the _reload_ icon):

image::inventory-codeready.png[Inventory RESTful Service, 700]

Open a *new* CodeReady Workspaces Terminal:

image::codeready-workspace-terminal.png[Inventory RESTful Service, 700]

and invoke the RESTful endpoint using the following CURL commands.

[source,sh,role="copypaste"]
----
curl http://localhost:8080/api/inventory | jq
----

The output looks like:

[source,json]
----
  ...
  {
    "id": 7,
    "itemId": "444435",
    "link": "http://maps.google.com/?q=Paris",
    "location": "Paris",
    "quantity": 600
  },
  {
    "id": 8,
    "itemId": "444437",
    "link": "http://maps.google.com/?q=Tokyo",
    "location": "Tokyo",
    "quantity": 230
  }
----

Be sure to terminate the running Quarkus development via kbd:[CTRL+C].

*In production*, the inventory service will connect to _PostgeSQL_ on OpenShift cluster.

We will use _Quarkus extension_ to add *PostgreSQL JDBC Driver*. Go back to CodeReady Workspaces Terminal and run the following
maven plugin:

[source,sh,role="copypaste"]
----
mvn -q quarkus:add-extension -Dextensions="jdbc-postgresql" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/inventory-service
----

You should see in the output:

[source,console]
----
✅ Extension io.quarkus:quarkus-jdbc-postgresql has been installed
----

First, open a new brower with the {{ CONSOLE_URL }}[OpenShift web console^]:

image::openshift_login.png[openshift_login, 700]

Login using:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

You will see a list of projects to which you have access:

image::openshift_landing.png[openshift_landing, 700]

[NOTE]
====
The project displayed in the landing page depends on which labs you will run today. If you will develop
`Service Mesh and Identity` then you will see pre-created projects as the above screeenshot.
====

Open the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^].

Our production inventory microservice will use an external database (PostgreSQL) to house inventory data. First, deploy a new
instance of PostgreSQL. Click *+Add* on the left, on the _Database_ box on the *{{ USER_ID }}-cloudnativeapps* project overview:

image::db.png[db, 700]

Type in `postgres` in the search box, and click on the *PostgreSQL (ephemeral)*:

image::db-postgres.png[db, 700]

Click on *Instantiate Template* and fill in the following fields, leaving the others as their default values:

* *Namespace*: _choose `{{ USER_ID }}-cloudnativeapps` for the first Namespace. Leave the second one as `openshift`_
* *Database Service Name*: `inventory-database`
* *PostgreSQL Connection Username*: `inventory`
* *PostgreSQL Connection Password*: `mysecretpassword`
* *PostgreSQL Database Name*: `inventory`

image::db-postgres-inventory-values.png[db, 700]

This will deploy the database to our new project. Click on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] to see it:

image::inventory-database-deployment.png[inventory_db_deployments, 700]


Although your CodeReady workspace is running on the Kubernetes cluster, it’s running with a default restricted _Service Account_ that prevents you from creating most resource types. If you’ve completed other modules, you’re probably already logged in, but let’s login again: click on *Login to OpenShift*, and enter your given credentials:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

image::cmd-login.png[login,700]

You should see something like this (the project names may be different):

[source,none]
----
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * {{ USER_ID }}-bookinfo
    {{ USER_ID }}-catalog
    {{ USER_ID }}-cloudnative-pipeline
    {{ USER_ID }}-cloudnativeapps
    {{ USER_ID }}-inventory
    {{ USER_ID }}-istio-system

Using project "{{ USER_ID }}-bookinfo".
Welcome! See 'oc help' to get started.
----

[NOTE]
====
After you log in using *Login to OpenShift*, the terminal is no longer usable as a regular terminal. You can close the terminal window. You will still be logged in when you open more terminals later!
====

Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

[source,sh,role="copypaste"]
----
oc project {{ USER_ID }}-cloudnativeapps && \
mvn clean compile package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/inventory-service
----

The output should end with `BUILD SUCCESS`.

Finally, make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/inventory
----

Wait for that command to report *replication controller _inventory-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/inventory app.kubernetes.io/part-of=inventory --overwrite && \
oc label dc/inventory-database app.kubernetes.io/part-of=inventory app.openshift.io/runtime=postgresql --overwrite && \
oc annotate dc/inventory app.openshift.io/connects-to=inventory-database --overwrite && \
oc annotate dc/inventory app.openshift.io/vcs-ref=ocp-4.6 --overwrite
----

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^], make sure it's done deploying (dark blue circle):

image::inventory_topology.png[inventory, 700]

Click on the small arrow icon as shown above and you will see inventories:

image::inventory_topology_openurl.png[inventory, 700]

So now `Inventory` service is deployed to OpenShift. You can also see it in the Project Status in the OpenShift Console with its single replica running in 1 pod, along with the Postgres database pod.

=== 2. Deploying Catalog Service

_Catalog Service_ serves products and prices for retail products. Lets’s go through quickly how the catalog service works and built on *Spring Boot* Java runtimes. Go to _Explorer: /projects_ in _CodeReady Workspaces_ Web IDE and expand *catalog-service* directory.

image::codeready-workspace-catalog-project.png[catalog, 700]

First of all, we won’t implement the catalog application to retrieve data because of all funtions are already built when we imported this project from Git server. There’re a few interesting things what we need to take a look at this Spring Boot
application before we will deploy it to OpenShift cluster.

This catalog service is not using the default BOM (Bill of material) that Spring Boot projects typically use. Instead, we are using a BOM provided by Red Hat as part of the http://snowdrop.me/[Snowdrop^] project.

[source,xml]
----
<dependency>
    <groupId>dev.snowdrop</groupId>
    <artifactId>snowdrop-dependencies</artifactId>
    <version>2.3.4.Final-redhat-00001</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
----

image::catalog-pom.png[catalog, 700]

Also, catalog service calls the inventory service that we deployed earlier using REST to retrieve the inventory status and include
that in the response. Open `CatalogService.java` in `src/main/java/com/redhat/cloudnative/service` directory via Project Explorer
and how `read()` and `readAll()` method work:

image::catalog-service-codes.png[catalog, 700]

Build and deploy the project using the following command, which will use the maven plugin to deploy via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package spring-boot:repackage -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/catalog-service
----

The build and deploy may take a minute or two. Wait for it to complete. You should see a `BUILD SUCCESS` at the end of the build output.

Our `production` catalog microservice will use an external database (PostgreSQL) to house inventory data. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^].

Click *+Add* on the left, on the _Database_ box on the project overview:

image::db.png[db, 700]

Type in `postgres` in the search box, and click on the *PostgreSQL (ephemeral)*:

image::db-postgres.png[db, 700]

Click on *Instantiate Template* and fill in the following fields, leaving the others as their default values:

* *Namespace*: _choose `{{ USER_ID }}-cloudnativeapps` for the first Namespace. Leave the second one as `openshift`_
* *Database Service Name*: `catalog-database`
* *PostgreSQL Connection Username*: `catalog`
* *PostgreSQL Connection Password*: `mysecretpassword`
* *PostgreSQL Database Name*: `catalog`

image::db-catalog-postgres-fields.png[db, 700]

This will deploy the database to our catalog project. Click on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] to see it.

Create a build configuration for your application using OpenJDK base container image in OpenShift:

[source, properties, role="copypaste"]
----
oc new-build registry.access.redhat.com/ubi8/openjdk-11 --binary --name=catalog -l app=catalog
----

Start and watch the build, which will take about minutes to complete:

[source,sh,role="copypaste"]
----
oc start-build catalog --from-file=$CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/catalog-service/target/catalog-1.0.0-SNAPSHOT.jar --follow
----

Once the build is done, we’ll deploy it as an OpenShift application and override the spring profile to use our _production_ values. We will also give it some labels to make it look nice. Run this command:

[source,sh,role="copypaste"]
----
oc new-app catalog -e JAVA_OPTS_APPEND='-Dspring.profiles.active=openshift' && oc expose service catalog && \
oc label dc/catalog app.kubernetes.io/part-of=catalog app.openshift.io/runtime=rh-spring-boot --overwrite && \
oc label dc/catalog-database app.kubernetes.io/part-of=catalog app.openshift.io/runtime=postgresql --overwrite && \
oc annotate dc/catalog app.openshift.io/connects-to=inventory,catalog-database --overwrite && \
oc annotate dc/catalog app.openshift.io/vcs-uri=https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs.git --overwrite && \
oc annotate dc/catalog app.openshift.io/vcs-ref=ocp-4.6 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the catalog, and ensure you get the blue circles! You will also see the icon of the *Red Hat Support for Spring Boot*.

image::inventory-catalog-topology.png[catalog, 700]

And then access the http://catalog-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Catalog Web frontend^] and ensure you get the expected inventory quantity (and not `-1`), you may need to reload the page if the app isn't initialized yet:

image::catalog.png[catalog, 700]

So now `Catalog` service is deployed to OpenShift. You can also see it in the Project Status in the OpenShift Console with running 4 pods such as catalog, catalog-database, inventory, and inventory-database.

=== 3. Developing and Deploying Shopping Cart Service

By now, you have deployed some of the essential elements for the Coolstore application. However, an online shop without a cart means no checkout experience. In this section, we are going to implement the Shopping Cart; in our Microservice world, we are going to call it the *cart service* and our java artifact/repo is called the *cart-service*.

The Cart service is RESTful and built with Quarkus using Red Hat’s Distributed _Data Grid_ technology. It stores all shopping cart data and assigns a unique id to each. It uses the Quarkus _Infinispan client_ to do this (_Infinispan_ is the name of the upstream project that Red Hat Data Grid is based on). The Shopping cart makes a call via the Quarkus REST client to fetch all items in the Catalog. In the end, Shopping cart also pushes messages to a _Kafka_ for each order, when the user is checking out. For that, we use the Quarkus Kafka client.

What is a _Shopping Cart_ in our context? A Shopping cart has a list of Shopping Items. Each item has a _quantity_, and other fields like discounts and promotional details. We will see these in more detail when we look at our model.

For this lab, we are using CodeReady Workspaces. Make sure you have the following project open in your workspace. Lets’s go through quickly how the cart service works and is built on _Quarkus_ Java runtimes. Go to _Explorer_ in CodeReady Workspaces and expand the *cart-service* directory.

image::codeready-workspace-cart-project.png[cart, 700]

We are going to use the Red Hat Distributed _Data Grid_ for caching all the users' carts.

Let's create a simple version of the *cache service* in our cluster. Open the Terminal in your CodeReady workspace and run the following command:

[source,sh,role="copypaste"]
----
oc new-app --as-deployment-config infinispan/server:12.0.0.Final-1 --name=datagrid-service -e USER=user -e PASS=pass
----

This will create a single instance of the Data Grid server to store our shopping carts.

Click on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] to see it.

Now that our cache service(a.k.a datagrid-service) is deployed. We want to ensure that everything in our cart is persisted in this blazing fast cache. It will help us when we have a few million users per second on a black Friday.

Following is what we need to do:

* Model our data
* Choose how we store the data
* Create a marshaller for our data
* Inject our cache connection into the service

We have made this choice easier for you using an annotation based serialization. Let’s take a look at our `Product.java` class file in `cart-service/src/main/java/com/redhat/cloudnative/model`:

[source,java]
----
...
    @ProtoFactory
    public Product(String itemId, String name, String desc, double price) {
        super();
        this.itemId = itemId;
        this.name = name;
        this.desc = desc;
        this.price = price;
    }

    @ProtoField(number = 1)
    public String getItemId() {
        return itemId;
    }
    public void setItemId(String itemId) {
        this.itemId = itemId;
    }
...
----

This can be done automatically by adding protostream annotations(*@ProtoFactory*, *@ProtoField*) to the _Product_ class. In addition a single _Initializer_ annotated interface is required which controls how the supporting classes are generated.

Then all that is required is a very simple *SerializationContextInitializer* interface with an annotation on it to specify configuration settings.

Create a new Java class called `CartContextInitializer.java` in `com.redhat.cloudnative.model` and copy the below code into the file:

[source,java,role="copypaste"]
----
package com.redhat.cloudnative.model;

import org.infinispan.protostream.SerializationContextInitializer;
import org.infinispan.protostream.annotations.AutoProtoSchemaBuilder;

@AutoProtoSchemaBuilder (includeClasses = {ShoppingCart.class, ShoppingCartItem.class, Promotion.class, Product.class }, schemaPackageName = "coolstore")
interface CartContextInitializer extends SerializationContextInitializer {

}
----

*Perfect!* Now we have all the building blocks ready to use the cache. Let's start using our cache.

Next we need to make sure we will inject our cache in our service. Open `com.redhat.cloudnative.service.ShoppingCartServiceImpl` and add this at the `// TODO Inject RemoteCache` marker:

[source,java,role="copypaste"]
----
    @Inject
    @Remote(CacheService.CART_CACHE)
    RemoteCache<String, ShoppingCart> carts;
----

The cart is quite simple; All the information from the browser i.e., via our *Angular App* is via _JSON_ at the _/api/cart_ endpoint:

* `GET {cartId}` gets the items in the cart, or creates a new unique ID if one is not present.
* `POST {cartId}/{itemId}/{quantity}` will add items to the cart.
* `DELETE {cartId}/{itemId}/{quantity}` will remove items from the cart.
* `POST checkout/{cartId}` will remove the items and invoke the checkout procedure.

Let’s take a look at how we do this with Quarkus. In our *cart-service* project and in our main package i.e., `com.redhat.cloudnative` is the `CartResource`.

At the `// TODO ADD getCart method` marker, add this method:

[source,java,role="copypaste"]
----
    public ShoppingCart getCart(@PathParam("cartId") String cartId) {
        return shoppingCartService.getShoppingCart(cartId);
    }
----

The code above is using the `ShoppingCartService`, which is injected into the `CartResource` via the Dependency Injection. The `ShoppingCartService` take a `cartId` as a parameter and returns the associated ShoppingCart. So that’s perfect, however, for our Endpoint i.e., CartResource to respond, we need to define a couple of things:

* The type of HTTPRequest
* The type of data it can receive
* The path it resolves too

Add the following code on top of the `getCart` method

[source,java,role="copypaste"]
----
    @GET
    @Produces(MediaType.APPLICATION_JSON)
    @Path("{cartId}")
----

We have now successfully stated that the method adheres to a GET request and accepts data in *plain text*. 

Take this opportunity to look at some of the other methods. You will find `@POST` and `@DELETE` and also the paths they adhere to. This is how we can construct a simple endpoint for our application.

[NOTE]
====
There are other *// TODO* markers and commented-out code we will use later. Leave them alone for now.
====

Quarkus also offers the ability to automatically generate OpenShift resources based on sane default and user supplied configuration. The OpenShift extension is actually a wrapper extension that brings together the [kubernetes](https://quarkus.io/guides/deploying-to-kubernetes) and [container-image-s2i](https://quarkus.io/guides/container-image#s2i) extensions with defaults so that it’s easier for the user to get started with Quarkus on OpenShift.

Add _openshift_ extension via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn -q quarkus:add-extension -Dextensions="openshift" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service
----

you will see:

✅ Extension io.quarkus:quarkus-openshift has been installed

Quarkus supports the notion of _configuration profiles_. These allows you to have multiple configurations in the same file and
select between then via a _profile name_.

Let’s `add` the following variables at the `# TODO: Add for OpenShift extension` marker in _src/main/resources/application.properties_:

[source,shell,role="copypaste"]
----
quarkus.kubernetes-client.trust-certs=true<1>
quarkus.container-image.build=true<2>
quarkus.kubernetes.deploy=true<3>
quarkus.kubernetes.deployment-target=openshift<4>
quarkus.openshift.expose=true<5>
quarkus.openshift.labels.app.openshift.io/runtime=quarkus<6>
----

<1> We are using self-signed certs in this simple example, so this simply says to the extension to trust them.
<2> Instructs the extension to build a container image
<3> Instructs the extension to deploy to OpenShift after the container image is built
<4> Instructs the extension to generate and create the OpenShift resources (like `DeploymentConfig` and `Service`) after building the container
<5> Instructs the extension to generate an OpenShift `Route`.
<6> Adds a nice-looking icon to the app when viewing the OpenShift Developer Toplogy

Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/cart-service
----

The output should end with `BUILD SUCCESS`.

Finally, make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/cart
----

Wait for that command to report *replication controller _cart-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/cart app.kubernetes.io/part-of=cart app.openshift.io/runtime=quarkus --overwrite && \
oc label dc/datagrid-service app.kubernetes.io/part-of=cart app.openshift.io/runtime=datagrid --overwrite && \
oc annotate dc/cart app.openshift.io/connects-to=catalog,datagrid-service --overwrite && \
oc annotate dc/cart app.openshift.io/vcs-ref=ocp-4.6 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the catalog, and ensure you get the blue circles!

image::cart-topology.png[catalog, 700]

And then access the http://cart-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN }}/swagger-ui[Cart Swagger UI^]:

image::cart-swagger-ui.png[cart, 700]

Notice that the documentation after the methods, this is an excellent way for other service developers to know what you intend to
do with each service method. You can try to invoke the methods and see the output from the service. Hence an excellent way to test
quickly as well.

=== 4. Developing and Deploying Order Service

The Order Service manages all orders when customers checkout items in the shopping cart. Lets’s go through quickly how the order
service get REST services to use the *MongoDB* database with *Quarkus* Java runtimes. Go to _Explorer: /projects_ in _CodeReady Workspaces_ Web IDE and expand *order-service* directory.

image::codeready-workspace-order-project.png[order, 700]

The application built in _Quarkus_ is quite simple: the user can add elements in a list using _RESTful APIs_ and the list is updated. All the information between the client and the server are formatted as *JSON*. The elements are stored in _MongoDB_.

Execute the following command for adding Maven Dependencies using Quarkus Extensions via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn -q quarkus:add-extension -Dextensions="resteasy-jsonb,mongodb-client" -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service
----

You should see in the output:

[source,console]
----
✅ Extension io.quarkus:quarkus-resteasy-jsonb has been installed
✅ Extension io.quarkus:quarkus-mongodb-client has been installed
----

This command generates a Maven structure importing the RESTEasy/JAX-RS, JSON-B and MongoDB Client extensions. After this, the quarkus-mongodb-client extension has been added to your *pom.xml*.

image::order-pom-dependency.png[order, 700]

Before we create the order service using JSON REST service, let's have a look at the `Order` bean in `src/main/java/com/redhat/cloudnative/` as follows:

image::order_bean.png[order, 700]

Nothing fancy. One important thing to note is that having a default constructor is required by the *JSON serialization layer*.

Now, open the `com.redhat.cloudnative.OrderService` class -- it will be the business layer of our application and _store/load_ the orders from the MongoDB database. Add the following java code at each marker.

`// TODO: Inject MongoClient here` marker:

[source,java,role="copypaste"]
----
    @Inject MongoClient mongoClient;
----

Next, add this code below the `// TODO: Add a while loop to make an order lists using MongoCursor here` marker (in the `list()` method).

[source,java,role="copypaste"]
----
        MongoCursor<Document> cursor = getCollection().find().iterator();

        try {
            while (cursor.hasNext()) {
                Document document = cursor.next();
                Order order = new Order();
                order.setOrderId(document.getString("orderId"));
                order.setName(document.getString("name"));
                order.setTotal(document.getString("total"));
                order.setCcNumber(document.getString("ccNumber"));
                order.setCcExp(document.getString("ccExp"));
                order.setBillingAddress(document.getString("billingAddress"));
                order.setStatus(document.getString("status"));
                list.add(order);
            }
        } finally {
            cursor.close();
        }
----

Next, add this code below the `// TODO: Add to create a Document based order here` marker in `add(Order order)` method:

[source,java,role="copypaste"]
----
        Document document = new Document()
                .append("orderId", order.getOrderId())
                .append("name", order.getName())
                .append("total", order.getTotal())
                .append("ccNumber", order.getCcNumber())
                .append("ccExp", order.getCcExp())
                .append("billingAddress", order.getBillingAddress())
                .append("status", order.getStatus());
        getCollection().insertOne(document);
----

These two methods convert between a `Document` object suitable for use with MongoDB and the `Order` document which is our business value object.

Now, edit the `com.redhat.cloudnative.OrderResource` class as follows in each marker:

`// TODO: Add JAX-RS annotations here` marker:

[source,java,role="copypaste"]
----
@Path("/api/orders")
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
----

`// TODO: Inject OrderService here` marker:

[source,java,role="copypaste"]
----
    @Inject OrderService orderService;
----

`// TODO: Add list(), add(), updateStatus() methods here` marker:

[source,java,role="copypaste"]
----
    @GET
    public List<Order> list() {
        return orderService.list();
    }

    @POST
    public List<Order> add(Order order) {
        orderService.add(order);
        return list();
    }

    @GET
    @Path("/{orderId}/{status}")
    public List<Order> updateStatus(@PathParam("orderId") String orderId, @PathParam("status") String status) {
        orderService.updateStatus(orderId, status);
        return list();
    }
----

The implementation is pretty straightforward and you just need to define your endpoints using the *JAX-RS annotations* and use the _OrderService_ to list/add new orders.

The main property to configure is the URL to access to *MongoDB*, almost all configuration can be included in the connection URI
so we advise you to do so, you can find more information in the
https://docs.mongodb.com/manual/reference/connection-string/[MongoDB documentation^]

Open `application.properties` in `src/main/resources/` and add the following configuration at the `# TODO: Add for MongoDB configuration` marker:

[source,sh,role="copypaste"]
----
quarkus.mongodb.connection-string = mongodb://order-database:27017
----

By using a Bson *Codec*, the MongoDB Client will take care of the transformation of your domain object to/from a MongoDB *Document* automatically.

First you need to create a Bson Codec that will tell Bson how to transform your entity to/from a MongoDB Document. Here we use a _CollectibleCodec_ as our object is retrievable from the database (it has a MongoDB identifier), if not we would have used a _Codec_ instead. More information in the https://mongodb.github.io/mongo-java-driver/3.10/bson/codecs[codec
documentation^].

Edit the `com.redhat.cloudnative.codec.OrderCodec` class as follows:

`// TODO: Add Encode & Decode contexts here` marker:

[source,java,role="copypaste"]
----
    @Override
    public void encode(BsonWriter writer, Order Order, EncoderContext encoderContext) {
        Document doc = new Document();
        doc.put("orderId", Order.getOrderId());
        doc.put("name", Order.getName());
        doc.put("total", Order.getTotal());
        doc.put("ccNumber", Order.getCcNumber());
        doc.put("ccExp", Order.getCcExp());
        doc.put("billingAddress", Order.getBillingAddress());
        doc.put("status", Order.getStatus());
        documentCodec.encode(writer, doc, encoderContext);
    }

    @Override
    public Class<Order> getEncoderClass() {
        return Order.class;
    }

    @Override
    public Order generateIdIfAbsentFromDocument(Order document) {
        if (!documentHasId(document)) {
            document.setOrderId(UUID.randomUUID().toString());
        }
        return document;
    }

    @Override
    public boolean documentHasId(Order document) {
        return document.getOrderId() != null;
    }

    @Override
    public BsonValue getDocumentId(Order document) {
        return new BsonString(document.getOrderId());
    }

    @Override
    public Order decode(BsonReader reader, DecoderContext decoderContext) {
        Document document = documentCodec.decode(reader, decoderContext);
        Order order = new Order();
        if (document.getString("orderId") != null) {
            order.setOrderId(document.getString("orderId"));
        }
        order.setName(document.getString("name"));
        order.setTotal(document.getString("total"));
        order.setCcNumber(document.getString("ccNumber"));
        order.setCcExp(document.getString("ccExp"));
        order.setBillingAddress(document.getString("billingAddress"));
        order.setStatus(document.getString("status"));
        return order;
    }
----

Then you need to create a `CodecProvider` to link this `Codec` to the Order class.

Edit the `com.redhat.cloudnative.codec.OrderCodecProvider` class as follows:

`// TODO: Add Codec get method here` marker:

[source,java,role="copypaste"]
----
    @Override
    public <T> Codec<T> get(Class<T> clazz, CodecRegistry registry) {
        if (clazz == Order.class) {
            return (Codec<T>) new OrderCodec();
        }
        return null;
    }
----

_Quarkus_ will register the _CodecProvider_ for you.

Finally, when getting the _MongoCollection_ from the database you can use directly the Order class instead of the Document one, the codec will automatically map the Document to/from your Order class.

Edit the `com.redhat.cloudnative.CodecOrderService` class as follows:

`// TODO: Add MongoCollection method here` marker:

[source,java,role="copypaste"]
----
    private MongoCollection<Order> getCollection(){
        return mongoClient.getDatabase("order").getCollection("order", Order.class);
    }
----

Run the following `oc` command to deploy a `MongoDB` to OpenShift via CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
oc new-app -n {{ USER_ID }}-cloudnativeapps --docker-image mongo:4.0 --name=order-database
----

Now let's deploy the application itself. Run the following command which will build and deploy using the OpenShift extension:

[source,sh,role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/order-service
----

The output should end with `BUILD SUCCESS`.

Finally, make sure it's actually done rolling out:

[source,sh,role="copypaste"]
----
oc rollout status -w dc/order
----

Wait for that command to report *replication controller _order-1_ successfully rolled out* before continuing.

And label the items with proper icons:

[source,sh,role="copypaste"]
----
oc label dc/order app.kubernetes.io/part-of=order --overwrite && \
oc label dc/order-database app.kubernetes.io/part-of=order app.openshift.io/runtime=mongodb --overwrite && \
oc annotate dc/order app.openshift.io/connects-to=order-database --overwrite && \
oc annotate dc/order app.openshift.io/vcs-ref=ocp-4.6 --overwrite
----

Finally, make sure it’s actually done rolling out. Visit the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] for the orders. Ensure you get the blue circles!

image::order-topology.png[order, 700]

And then access the http://order-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}/api/orders[Orders^]. You will see empty result because you didn’t add any shopping items yet:

[source,sh]
----
[]
----

You can also see this with `curl` with this command in a Terminal:

[source,sh,role="copypaste"]
----
curl -s http://order-{{USER_ID}}-cloudnativeapps.{{ROUTE_SUBDOMAIN}}/api/orders | jq
----

Which will also return an empty array `[]`.

=== 5. Deploying WEB-UI Service

Our Web UI serves a frontend based on https://angularjs.org/[AngularJS^] and http://patternfly.org/[PatternFly^] running in a https://access.redhat.com/documentation/en/openshift-container-platform/3.3/paged/using-images/chapter-2-source-to-image-s2i[Node.js] container. https://www.redhat.com/en/products/runtimes[Red Hat Runtimes^] includes *Node.js* support along with other runtimes used for cloud native development.

Lets’s go through quickly how the frontend service works and is built on Node.js runtimes. Go to _Explorer: /projects_ in CodeReady Workspaces and expand the `coolstore-ui` directory.

image::codeready-workspace-coolstore-ui.png[coolstore-ui, 700]

You will see javascript for specific cloud-native services such as the cart, catatlog, and order service as above.

Now, we will deploy a presentation layer to OpenShift cluster using https://www.npmjs.com/package/nodeshift[Nodeshift] command line tool, a programmable API that you can use to deploy Node.js projects to OpenShift.

Install Nodeshift via the CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
cd $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/coolstore-ui && npm install --save-dev nodeshift
----

[NOTE]
====
You may see warnings from _npm_ about known vulnerabilities. The _npm_ ecosystem is huge and our app pulls in many dependencies which are constantly finding new issues. In a real production system you should pay attention to these, but for this workshop you can ignore them (your friendly workshop developers fix them as time permits when we're not supporting our customers!).
====

Next, deploy the _coolstore-ui_ service using `Nodeshift` in a CodeReady Workspaces Terminal. It will take a minute to complete the deployment:

[source,sh,role="copypaste"]
----
npm run nodeshift && oc expose svc/coolstore-ui && \
oc label dc/coolstore-ui app.kubernetes.io/part-of=coolstore --overwrite && \
oc annotate dc/coolstore-ui app.openshift.io/connects-to=order-cart,catalog,inventory,order --overwrite && \
oc annotate dc/coolstore-ui app.openshift.io/vcs-uri=https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs.git --overwrite && \
oc annotate dc/coolstore-ui app.openshift.io/vcs-ref=ocp-4.6 --overwrite
----

Back on the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^], make sure it's done deploying (dark blue circle):

image::coolstore-ui_topology.png[coolstore-ui, 700]

And then access the http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^] and ensure you get the expected products and inventories:

image::web-ui-landing.png[coolstore-ui, 700]

This confirms that the frontend is properly hooked up to the backend, which is properly hooked up to our Data Grid deployment.

=== Summary

In this scenario we developed and deployed 5 microservices, each with a REST API and each of which communicate with the other microservices. We also used a variety of application runtimes such as Quarkus, Spring Boot, and Node.js to compile, package, and containerize applications -- this is an important capability of advanced cloud-native architectures.

To deploy cloud-native applications with multiple datasources on an OpenShift cluster, Quarkus provides an easy way to connect multiple datasources and obtain a reference to those datasources such as PostgreSQL and MongoDB in code.

In the end, we optimized the _transaction performance_ of the shopping cart service by integrating it with *Red Hat Data Grid* to increase end users’ (customers) satification. This may not be obvious with only one user (you), but at scale these components can ensure relability and business performance. *Congratulations!*
